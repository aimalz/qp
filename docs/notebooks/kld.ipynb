{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kullback-Leibler Divergence\n",
    "\n",
    "The KL divergence is used as a measure of how close an approximation to a probability distribution is to the true probability distribution it approximates.  In this notebook, we try to gain some intuition about the magnitude of the KL divergence by computing its value between two Gaussian PDFs as a function of the \"precision\" and \"tension\" between them.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You'll need the `qp` package and its dependencies (notably `scipy` and matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Kullback-Leibler divergence between probability distributions $P$ and $Q$ is:\n",
    "\n",
    "$D(P||Q) = \\int_{-\\infty}^{\\infty} \\log \\left( \\frac{P(x)}{Q(x)} \\right) P(x) dx$\n",
    "\n",
    "The Wikipedia page for the KL divergence gives the following useful interpretation of the KLD:\n",
    "\n",
    "> KL divergence is a measure of the difference between two probability distributions $P$ and $Q$. It is not symmetric in $P$ and $Q$. In applications, $P$ typically represents ... a precisely calculated theoretical distribution, while $Q$ typically represents ... [an] approximation of $P$.\n",
    ">\n",
    "> Specifically, the Kullback–Leibler divergence from $Q$ to $P$, denoted $D_{KL}(P‖Q)$, is a measure of the information gained when one revises one's beliefs from ... $Q$ to ... $P$. In other words, it is the amount of information lost when $Q$ is used to approximate $P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Gaussian Illustration\n",
    "\n",
    "\"Information\" is not a terribly familiar quantity to most of us, so let's compute the KLD between two Gaussians:\n",
    "\n",
    "* The \"True\" 1D Gaussian PDF, $P(x)$, of unit width and central value 0\n",
    "\n",
    "* An \"approximating\" 1D Gaussian PDF, $Q$, of width $\\sigma$ and centroid $x$\n",
    "\n",
    "How does the KLD between these PDFs vary with offset $x$ and width $\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = qp.PDF(truth=sps.norm(loc=0.0, scale=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, sigma = 2.0, 1.0\n",
    "Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two equal-width Gaussians overlapping at their 1-sigma points have a KLD of 2 nats. \n",
    "\n",
    "> The unit of information here is a \"nat\" rather than a \"bit\" because `qp` uses a natural logarithm in its KLD calculation. 1 nat = $1/\\log{2} \\approx 1.44$  bits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the two Gaussians are perfectly aligned, but the approximation is broader than the truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, sigma = 0.0, 4.37\n",
    "Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))\n",
    "D = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Two concentric 1D Gaussian PDFs differing in width by a factor of 4.37 have a KLD of 1 nat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Formulae\n",
    "\n",
    "It is illustrative to consider the KL divergence (in nats) between an approximating Gaussian of mean $\\mu$ and variance $\\sigma^{2}$ to a true Gaussian of mean $\\mu_{0}$ and variance $\\sigma_{0}^{2}$.\n",
    "\n",
    "\\begin{align*}\n",
    "D &= \\int_{-\\infty}^{\\infty}\\ P(x)\\ \\log\\left[\\frac{P(x)}{Q(x)}\\right]\\ dx\\\\\n",
    "&= \\int_{-\\infty}^{\\infty}\\ \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\ \\log\\left[\\frac{\\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right]}\\right]\\ dx\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\int_{-\\infty}^{\\infty}\\ \\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\ \\left(\\log\\left[\\frac{\\sigma}{\\sigma_{0}}\\right]-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}+\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\\ dx\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\left(\\log\\left[\\frac{\\sigma}{\\sigma_{0}}\\right]\\int_{-\\infty}^{\\infty}\\ \\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\ dx-\\int_{-\\infty}^{\\infty}\\ \\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\ \\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\ dx+\\int_{-\\infty}^{\\infty}\\ \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\ \\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\ dx\\right)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\left(\\log\\left[\\frac{\\sigma}{\\sigma_{0}}\\right]\\left(-\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}\\ erf\\left[\\frac{\\mu_{0}-x}{\\sqrt{2}\\sigma_{0}}\\right]\\right)|_{-\\infty}^{\\infty}-\\left(\\frac{\\mu_{0}-x}{2}\\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]-\\frac{1}{2}\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}\\ erf\\left[\\frac{\\mu_{0}-x}{\\sqrt{2}\\sigma_{0}}\\right]\\right)|_{-\\infty}^{\\infty}+\\left(-\\frac{1}{2\\sigma^{2}}\\left(\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}((\\mu_{0}-\\mu)^{2}+\\sigma_{0}^{2})\\ erf\\left[\\frac{\\mu_{0}-x}{\\sqrt{2}\\sigma_{0}}\\right]+\\sigma_{0}^{2}(x-\\mu+\\mu_{0}-\\mu)\\exp\\left[-\\frac{(x-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\right)\\right)|_{-\\infty}^{\\infty}\\right)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_{0}}\\left(\\log\\left[\\frac{\\sigma}{\\sigma_{0}}\\right]\\left(2\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}\\right)-0+2\\frac{1}{2}\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}\\right)+\\left(-\\frac{1}{2\\sigma^{2}}\\left(-2\\sqrt{\\frac{\\pi}{2}}\\sigma_{0}((\\mu_{0}-\\mu)^{2}+\\sigma_{0}^{2})+0\\right)\\right)\\\\\n",
    "&= \\log\\left[\\frac{\\sigma}{\\sigma_{0}}\\right]-\\frac{1}{2}+\\frac{1}{2}\\left(\\frac{(\\mu_{0}-\\mu)^{2}}{\\sigma^{2}}+\\frac{\\sigma_{0}^{2}}{\\sigma^{2}}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We note that when $\\mu=\\mu_{0}$ and $\\sigma^{2}=\\sigma_{0}^{2}$, we have $D=0$, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "From the above, it is natural to define the \"precision\" as $\\alpha\\equiv\\frac{\\sigma_{0}}{\\sigma}$.  This corresponds to the increase in precision going from the approximation to the truth.  The KL divergence then satisfies $D\\sim\\log\\alpha+\\alpha^{-2}$.  In the limit of $\\sigma>\\sigma_{0}$, we will have $\\alpha\\sim D^{-1/2}$.  In the limit of $\\sigma<\\sigma_{0}$, we will have $\\alpha\\sim\\exp[D]$.  Precision lost when using the approximation is then proportional to the exponential of the KL divergence for an approximation more restrictive than the truth and the inverse square root of the KL divergence for an approximation less restrictive than the truth.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tension\n",
    "\n",
    "It is also natural to define the \"tension\" as $t\\equiv\\frac{|\\mu_{0}-\\mu|}{\\sqrt{\\left(\\sigma_0^2 + \\sigma^2\\right)}}$.  We can rewrite the KL divergence as $D\\sim t^{2}$, so $t\\sim\\sqrt{D}$.  Since has, in some sense, \"units\" of \"$\\sigma$\", the KL divergence is the information lost when using the approximation: the information loss rises in proprtion to the tension squared. We can see that the KL divergence might provide a route to a generalized quantification of tension.  The square root of the KL divergence between a true distribution and its approximation, in nats, gives an approximate sense of the tension between the two distributions, in \"units\" of \"$\\sigma$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation Precision\n",
    "\n",
    "Suppose our approximating PDF is broader than the true PDF, but the centroids are aligned. If we were to use our approximation, we'd be over-estimating the uncertainty in the inference. The approximating PDF represents a lower _precision_ measurement. Let's look at how the KLD quantifies this change in precision, as a function of the change in PDF width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widths = np.logspace(-3.0,3.0,13)\n",
    "D = np.empty_like(widths)\n",
    "\n",
    "x = 0.0\n",
    "infinity = 1000.0\n",
    "\n",
    "for k,sigma in enumerate(widths):\n",
    "    Q = qp.PDF(truth=sps.norm(loc=x, scale=sigma))\n",
    "    D[k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "    \n",
    "print zip(widths, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = widths\n",
    "y = np.log(widths)+0.5*widths**-2-0.5\n",
    "plt.plot(x, y, color='gray', linestyle='-', lw=8.0, alpha=0.5, label=r'$\\log[\\sigma]+\\frac{1}{2}\\sigma^{-2}$-\\frac{1}{2}')\n",
    "\n",
    "plt.plot(widths, D, color='black', linestyle='-', lw=2.0, alpha=1.0, label='Offset=0.0')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0.0,32.0)\n",
    "plt.xlabel('Width of approximating Gaussian $\\sigma$')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though using an increasingly broad approximation distribution leads to logarithmically increasing information loss. When the approximating distribution gets _narrower_ than the truth, information is lost at a faster rate, which is interesting.  The mismatch at low $\\sigma$ may be due to floating point precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tension between PDFs\n",
    "\n",
    "Two measurements that disagree with each other will lead to parameter PDFs that have different cenrtroids. Let's tabulate the KLD for a range of distribution offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "separations = np.linspace(0.0,15.0,16)\n",
    "D = np.empty_like(separations)\n",
    "\n",
    "sigma = 1.0\n",
    "infinity = 100.0\n",
    "\n",
    "for k,x0 in enumerate(separations):\n",
    "    Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "    D[k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "    \n",
    "print zip(separations, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(separations, D, color='k', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.plot(separations, 0.5*separations**2-0.5, color='gray', linestyle='-', lw=8.0, alpha=0.5, label=r'$\\frac{1}{2}\\frac{\\mu}{\\sqrt{\\sigma^{2}+1}}-\\frac{1}{2}$')\n",
    "plt.xlabel('Separation between Gaussians')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For separations greater than about 7 sigma, numerical precision starts to matter: the overlap integral out here is smaller than machine precision. `qp` uses a `safelog` function that replaces values smaller than the system threshold value with that threshold; the log of that threshold is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print np.log(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Probably the precision analysis of the previous section suffered from the same type of numerical error, at very low approximation distribution widths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably what matters is the \"tension\" between the two distributions: the separation in units of the combined width, $\\sqrt{\\sigma_0^2 + \\sigma^2}$. This quantity comes up often in discussions of dataset combination, where the difference in centroids of two posterior PDFs needs to be expressed in terms of their widths: the quadratic sum makes sense in this context, since it would appear in the product of the two likelihoods were they to be combined.\n",
    "\n",
    "For a few different widths, let's plot KLD vs tension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infinity = 100.0\n",
    "widths = np.array([1.0,1.5,2.0,2.5,3.0,3.5,4.0]) \n",
    "separations = np.linspace(0.0,7.0,15)\n",
    "\n",
    "D = np.zeros([7,len(separations)])\n",
    "tensions = np.empty_like(D)\n",
    "\n",
    "for j,sigma in enumerate(widths):\n",
    "    \n",
    "    for k,x0 in enumerate(separations):\n",
    "        Q = qp.PDF(truth=sps.norm(loc=x0, scale=sigma))\n",
    "        D[j,k] = qp.utils.calculate_kl_divergence(P, Q, limits=(-infinity,infinity), vb=False)\n",
    "        tensions[j,k] = x0 / np.sqrt(sigma*sigma + 1.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tensions[0,:]\n",
    "y = x**2\n",
    "plt.plot(x, y, color='gray', linestyle='-', lw=8.0, alpha=0.5, label='$t^2$')\n",
    "\n",
    "plt.plot(tensions[0,:], D[0,:], color='black', linestyle='-', lw=2.0, alpha=1.0, label='Width=1.0')\n",
    "plt.plot(tensions[1,:], D[1,:], color='violet', linestyle='-', lw=2.0, alpha=1.0, label='Width=1,5')\n",
    "plt.plot(tensions[2,:], D[2,:], color='blue', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.0')\n",
    "plt.plot(tensions[3,:], D[3,:], color='green', linestyle='-', lw=2.0, alpha=1.0, label='Width=2.5')\n",
    "plt.plot(tensions[4,:], D[4,:], color='yellow', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.0')\n",
    "plt.plot(tensions[5,:], D[5,:], color='orange', linestyle='-', lw=2.0, alpha=1.0, label='Width=3.5')\n",
    "plt.plot(tensions[6,:], D[6,:], color='red', linestyle='-', lw=2.0, alpha=1.0, label='Width=4.0')\n",
    "plt.xlabel('Tension between Gaussians, $t$ (sigma)')\n",
    "plt.ylabel('KL divergence (nats)')\n",
    "l = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "To summarize, the KL divergence $D$ is an appropriate metric of an approximation to a probability distribution, expressing the loss of information of the approximation from the true distribution.  The simple numerical experiments in this notebook suggest the following approximate extrapolations and hypotheses.  \n",
    "\n",
    "Using a Gaussian example enables exploration of two quantities characterizing the approximate distribution: the \"precision\" $\\alpha$ is a measure of the width of the approximating distribution relative to the truth, and the \"tension\" $t$ is a measure of the difference in centroids weighted by the root-mean-square width of the two distributions.  We have found that the KLD can be interpreted in terms of these quantities; the KLD is proportional to the log of the precision and the square of the tension.\n",
    "\n",
    "We can also think about when these approximations come up in photo-$z$ PDF expression.  If the true distribution contains small features that are not recoverable from the parametric representation of the distribution, it corresponds to a loss of precision.  This could happen if a binned parametrization has bins that are broader than the features.  If feature location information is lost when parametrizing the distribution, it can result in tension between the truth and the parametrization.  This could happen if a binned parametrization is based on samples from the truth with bins narrower than the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
