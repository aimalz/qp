{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `qp` Demo\n",
    "\n",
    "_Alex Malz & Phil Marshall_\n",
    "\n",
    "In this notebook we use the `qp` module to approximate some simple, standard, 1-D PDFs using sets of quantiles, samples, and histograms, and assess their relative accuracy. We also show how such analyses can be extended to use \"composite\" PDFs made up of mixtures of standard distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To run `qp`, you will need to first install the module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.interpolate as spi\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `qp.Parametrization` class\n",
    "\n",
    "A parametrization is defined by a format, metaparameters, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(qp.Parametrization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qp` includes four base parametrizations: `FuncForm`, `PointEval`, `Quantile`, and `Sample`.  Each corresponds to a `format_type`: `funcform`, `pointeval`, `quantile`, and `sample`, respectively.  The metaparameters and parameters are dictionaries, and their keys depend on the format.\n",
    "\n",
    "|   | metaparameters | parameters | notes |\n",
    "| - | -------------- | ---------- | ----- |\n",
    "| `FuncForm` | `function`, `**kwargs` | `**kwargs` | `function` must be instantiated as `function(**kwargs)`; `function` must support `.cdf`, `.fit`, `.pdf`, `.ppf`, and `.rvs`\n",
    "| `PointEval` | `x` | `y` |  |\n",
    "| `Quantile` | `q` | `x` |\n",
    "| `Sample` | None | `x` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_metaparameters = {'function': sps.norm}\n",
    "dummy_parameters = {'loc': 0., 'scale': 1.}\n",
    "F = qp.FuncForm(dummy_metaparameters, dummy_parameters, name='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All conversions between these four base formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP = F.convert('pointeval', np.linspace(-2., 2., 100))\n",
    "FQ = F.convert('quantile', np.linspace(0., 1., 100)[1:-1])\n",
    "FS = F.convert('sample', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even convert a parametrization to the same format but different metaparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FFvP_info = {'format_type':'pointeval', 'metaparam':FP.metaparam}\n",
    "FFvP = F.convert('funcform', dummy_metaparameters, via=FFvP_info, vb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPF = FP.convert('funcform', dummy_metaparameters)\n",
    "FPFP = FPF.convert('pointeval', np.linspace(-2., 2., 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(FPFP.metaparam, FPFP.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_sample = {'format_type': 'sample', 'metaparam': 100}\n",
    "# FF2 = FF.convert('funcform', dummy_metaparameters, name='normal2', via=intermediate_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional `qp.Parametrization` subclasses may be defined by a user, but `FuncForm` is in many cases sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(PE.metaparam, PE.param)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `qp.ProbDist` Class\n",
    "\n",
    "This is the basic element of `qp` -- an object representing a probability density function. This class is stored in the module `probdist.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = qp.ProbDist(vb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! cat qp/pdf.py\n",
    "P = qp.PDF(vb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating a Gaussian\n",
    "\n",
    "Let's summon a PDF object, and initialize it with a standard function - a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = sps.norm(loc=0, scale=1)\n",
    "print(type(dist))\n",
    "demo_limits = (-5., 5.)\n",
    "P = qp.PDF(funcform=dist, limits=demo_limits)\n",
    "P.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "\n",
    "Let's sample the PDF to see how it looks.  When we plot the `PDF` object, both the true and sampled distributions are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "samples = P.sample(1000, using='mix_mod', vb=False)\n",
    "S = qp.PDF(samples=samples, limits=demo_limits)\n",
    "S.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Parametrization\n",
    "\n",
    "Now, let's compute a set of evenly spaced quantiles. These will be carried by the `PDF` object as `p.quantiles`.  We also demonstrate the initialization of a `PDF` object with quantiles and no truth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantiles = P.quantize(N=10)\n",
    "Q = qp.PDF(quantiles=quantiles, limits=demo_limits)\n",
    "Q.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Parametrization\n",
    "\n",
    "Let's also compute a histogram representation, that will be carried by the `PDF` object as `p.histogram`. The values in each bin are the integrals of the PDF over the range defined by bin ends. We can also initialize a `PDF` object with a histogram and no truth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = P.histogramize(N=10, binrange=demo_limits)\n",
    "H = qp.PDF(histogram=histogram, limits=demo_limits)\n",
    "H.plot()\n",
    "print H.truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Approximate PDF by Interpolation\n",
    "\n",
    "Once we have chosen a parametrization to approximate the PDF with, we can evaluate the approximate PDF at any point by interpolation (or extrapolation). `qp` uses [`scipy.intepolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html) to do this, with `linear` as the default interpolation scheme. (Most other options do not enable extrapolation, `nearest` being the exception.)\n",
    "\n",
    "Let's test this interpolation by evaluating an approximation at a single point using the quantile parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print P.approximate(np.array([0.314]), using='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.mix_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We can also integrate any approximation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print P.integrate([0., 1.], using='quantiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also interpolate the function onto an evenly spaced grid with points within and out of the quantile range, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(-3., 3., 100)\n",
    "gridded = P.approximate(grid, using='quantiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the interpolation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print P.scheme\n",
    "print P.approximate(np.array([0.314]), using='quantiles', scheme='nearest')\n",
    "print P.scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Evaluated\" or \"Gridded\" Parametrization\n",
    "\n",
    "A `qp.PDF` object may also be initialized with a parametrization of a function evaluated on a grid. This is also what is produced by the `qp.PDF.approximate()` method. So, let's take the output of a `qp.PDF` approximation evaluation, and use it to instantiate a new `qp.PDF` object. Note that the `evaluate` method can be used to return PDF evaluations from either the true PDF or one of its approximations, via the `using` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(-3., 3., 20)\n",
    "gridded = P.evaluate(grid, using='mix_mod', vb=False)\n",
    "\n",
    "G = qp.PDF(gridded=gridded, limits=demo_limits)\n",
    "G.sample(100, vb=False)\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this a little. The `G` PDF object has an attribute `G.gridded` which contains the initial gridded function. This lookup table is used when making further approximations. To check this, let's look at whether this `G` PDF object knows what the _true_ PDF is, which approximation it's going to use, and then how it performs at making a new approximation to the PDF on a coarser grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print G.truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print G.last,'approximation, ', G.scheme, 'interpolation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-point grid for a coarse approximation:\n",
    "coarse_grid = np.linspace(-3.5, 3.5, 10)\n",
    "coarse_evaluation = G.approximate(coarse_grid, using='gridded')\n",
    "print coarse_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Model Fit\n",
    "\n",
    "We can fit a parametric mixture model to samples from any parametrization.  Currently, only a Gaussian mixture model is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MM = qp.PDF(funcform=dist, limits=demo_limits)\n",
    "MM.sample(1000, vb=False)\n",
    "MM.mix_mod_fit(n_components=5)\n",
    "MM.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Parametrizations\n",
    "\n",
    "`qp` supports both qualitative and quantitative comparisons between different distributions, across parametrizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Comparisons: Plotting\n",
    "\n",
    "Let's visualize the PDF object in order to compare the truth and the approximations.  The solid, black line shows the true PDF evaluated between the bounds.  The green rugplot shows the locations of the 1000 samples we took.  The vertical, dotted, blue lines show the percentiles we asked for, and the hotizontal, dotted, red lines show the 10 equally spaced bins we asked for.  Note that the quantiles refer to the probability distribution *between the bounds*, because we are not able to integrate numerically over an infinite range. Interpolations of each parametrization are given as dashed lines in their corresponding colors.  Note that the interpolations of the quantile and histogram parametrizations are so close to each other that the difference is almost imperceptible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symm_lims = np.array([-1., 1.])\n",
    "all_lims = [symm_lims, 2.*symm_lims, 3.*symm_lims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, let's compare the different parametrizations to the truth using the Kullback-Leibler Divergence (KLD).  The KLD is a measure of how close two probability distributions are to one another -- a smaller value indicates closer agreement.  It is measured in units of bits of information, the information lost in going from the second distribution to the first distribution.  The KLD calculator here takes in a shared grid upon which to evaluate the true distribution and the interpolated approximation of that distribution and returns the KLD of the approximation relative to the truth, which is not in general the same as the KLD of the truth relative to the approximation.  Below, we'll calculate the KLD of the approximation relative to the truth over different ranges, showing that it increases as it includes areas where the true distribution and interpolated distributions diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for PDF in [Q, H, S]:\n",
    "    D = []\n",
    "    for lims in all_lims:\n",
    "        D.append(qp.metrics.calculate_kld(P, PDF, limits=lims, vb=False))\n",
    "    print(PDF.truth+' approximation: KLD over 1, 2, 3, sigma ranges = '+str(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Holy smokes, does the quantile approximation blow everything else out of the water, thanks to using spline interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progression of KLD values should follow that of the root mean square error (RMSE), another measure of how close two functions are to one another.  The RMSE also increases as it includes areas where the true distribution and interpolated distribution diverge.  Unlike the KLD, the RMSE is symmetric, meaning the distance measured is not that of one distribution from the other but of the symmetric distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for PDF in [Q, H, S]:\n",
    "    D = []\n",
    "    for lims in all_lims:\n",
    "        D.append(qp.metrics.calculate_rmse(P, PDF, limits=lims, vb=False))\n",
    "    print(PDF.truth+' approximation: RMSE over 1, 2, 3, sigma ranges = '+str(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the KLD and RMSE metrics suggest that the quantile approximation is better in the high density region, but samples work better when the tails are included. We might expect the answer to the question of which approximation to use to depend on the application, and whether the tails need to be captured or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the meoments of each approximation and compare those to the moments ofthe true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = [P, Q, H, S]\n",
    "which_moments = range(3)\n",
    "all_moments = []\n",
    "for pdf in pdfs:\n",
    "    moments = []\n",
    "    for n in which_moments:\n",
    "        moments.append(qp.metrics.calculate_moment(pdf, n))\n",
    "    all_moments.append(moments)\n",
    "    \n",
    "print('moments: '+str(which_moments))\n",
    "for i in range(len(pdfs)):\n",
    "    print(pdfs[i].first+': '+str(all_moments[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three moments have an interesting interpretation.  The zeroth moment should always be 1 when calculated over the entire range of redshifts, but the quantile approximation is off by about $7\\%$.  We know the first moment in this case is 0, and indeed the evaluation of the first moment for the true distribution deviates from 0 by less than Python's floating point precision.  The samples parametrization has a biased estimate for the first moment to the tune of $2\\%$.  The second moment for the true distribution is 1, and the quantile parametrization (and, to a lesser extent, the histogram parametrization) fails to provide a good estimate of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite PDFs\n",
    "\n",
    "In addition to individual `scipy.stats.rv_continuous` objects, `qp` can be initialized with true distributions that are linear combinations of `scipy.stats.rv_continuous` objects.  To do this, one must create the component distributions and specify their relative weights.  This can be done by running `qp.PDF.mix_mod_fit()` on an existing `qp.PDF` object once samples have been calculated, or it can be done by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_1 = {}\n",
    "component_1['function'] = sps.norm(loc=-2., scale=1.)\n",
    "component_1['coefficient'] = 4.\n",
    "component_2 = {}\n",
    "component_2['function'] = sps.norm(loc=2., scale=1.)\n",
    "component_2['coefficient'] = 1.\n",
    "dist_info = [component_1, component_2]\n",
    "\n",
    "composite_lims = (-5., 5.)\n",
    "\n",
    "C_dist = qp.composite(dist_info)\n",
    "C = qp.PDF(funcform=C_dist, limits=composite_lims)\n",
    "C.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the quantiles for such a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cq = qp.PDF(funcform=C_dist, limits = composite_lims)\n",
    "Cq.quantize(N=20, limits=composite_lims, vb=False)\n",
    "Cq.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the histogram parametrization is also supported for composite PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ch = qp.PDF(funcform=C_dist, limits = composite_lims)\n",
    "Ch.histogramize(N=20, binrange=composite_lims, vb=True)\n",
    "Ch.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, samples from this distribution may also be taken, and a PDF may be reconstructed from them.  Note: this uses [`scipy.stats.gaussian_kde`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html), which determines its bandwidth/kernel size using Scott's Rule, Silverman's Rule, a fixed bandwidth, or a callable function that returns a bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = qp.PDF(funcform=C_dist, limits = composite_lims)\n",
    "Cs.sample(N=20, using='mix_mod', vb=False)\n",
    "Cs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qD = qp.metrics.calculate_kld(C, Cq, limits=composite_lims, dx=0.001, vb=True)\n",
    "hD = qp.metrics.calculate_kld(C, Ch, limits=composite_lims, dx=0.001, vb=True)\n",
    "sD = qp.metrics.calculate_kld(C, Cs, limits=composite_lims, dx=0.001, vb=True)\n",
    "print(qD, hD, sD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PDF Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "in_dists = []\n",
    "for i in range(N):\n",
    "    dist = sps.norm(loc=sps.uniform.rvs(), scale=sps.uniform.rvs())\n",
    "    in_dists.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = qp.Ensemble(N, funcform=in_dists, vb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_range = np.linspace(-5., 5., 100)\n",
    "E.evaluate(eval_range, using='mix_mod', vb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.quantize(N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.integrate(demo_limits, using='mix_mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "`qp` does not by default support \"stacking,\" a popular practice in the world of photo-$z$s, because of the risk of misuse in science applications.  However, it is easy to write your own function that performs the procedure, such as the one below, if it is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(ensemble, loc, using, vb=True):\n",
    "    \"\"\"\n",
    "    Produces an average of the PDFs in the ensemble\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ensemble: qp.Ensemble\n",
    "        the ensemble of PDFs to stack\n",
    "    loc: ndarray, float or float\n",
    "        location(s) at which to evaluate the PDFs\n",
    "    using: string\n",
    "        which parametrization to use for the approximation\n",
    "    vb: boolean\n",
    "        report on progress\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stacked: dict, tuple, ndarray, float\n",
    "        pair of arrays for locations where approximations were evaluated\n",
    "        and the values of the stacked PDFs at those points\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Stacking refers to taking the sum of PDFs evaluated on a shared grid and normalizing it such that it integrates to unity.  This is equivalent to calculating an average probability (based on the PDFs in the ensemble) over the grid.  This probably should be done in a script and not by qp!  The right way to do it would be to call qp.Ensemble.evaluate() and sum those outputs appropriately.\n",
    "    TO DO: make this do something more efficient for mixmod, grid, histogram, samples\n",
    "    \"\"\"\n",
    "#     loc_range = max(loc) - min(loc)\n",
    "#     delta = loc_range / len(loc)\n",
    "#     loc_delta = loc[1:] - loc[:-1]\n",
    "    evaluated = ensemble.evaluate(loc, using=using, norm=True, vb=vb)\n",
    "    stack = np.mean(evaluated[1], axis=0)\n",
    "#     stack /= np.sum(stack) * loc_delta\n",
    "#     assert(np.isclose(np.sum(stack) * loc_delta, 1.))\n",
    "#     stacked[using] = (evaluated[0], stack)\n",
    "    return (evaluated[0], stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = stack(E, eval_range, using='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stacked[0], stacked[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (not)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
