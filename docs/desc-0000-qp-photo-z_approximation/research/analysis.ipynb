{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Analysis Pipeline\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we use the \"survey mode\" machinery to demonstrate how one should choose the optimal parametrization for photo-$z$ PDF storage given the nature of the data, the storage constraints, and the fidelity necessary for a science use case.\n",
    "\n",
    "_This notebook is intended to work with [qp v0.2-beta](https://zenodo.org/badge/latestdoi/73841220) and may not work with other versions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment out for NERSC\n",
    "%load_ext autoreload\n",
    "\n",
    "#comment out for NERSC\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import pickle\n",
    "import hickle\n",
    "import numpy as np\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "import bisect\n",
    "import re\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "# np.random.seed(seed=42)\n",
    "# random.seed(a=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "#comment out for NERSC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataset(dataset_key, skip_rows, skip_cols):\n",
    "    start = timeit.default_timer()\n",
    "    with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "        lines = (line.split(None) for line in data_file)\n",
    "        for r in range(skip_rows):\n",
    "            lines.next()\n",
    "        pdfs = np.array([[float(line[k]) for k in range(skip_cols, len(line))] for line in lines])\n",
    "    print('read in data file in '+str(timeit.default_timer()-start))\n",
    "    return(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_instantiation(dataset_key, n_gals_use, pdfs, bonus=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    n_gals_tot = len(pdfs)\n",
    "    full_gal_range = range(n_gals_tot)\n",
    "    subset = np.random.choice(full_gal_range, n_gals_use, replace=False)#range(n_gals_use)\n",
    "#     subset = indices\n",
    "    print('randos for debugging: '+str(subset))\n",
    "    pdfs_use = pdfs[subset]\n",
    "    \n",
    "    modality = []\n",
    "    dpdfs = pdfs_use[:,1:] - pdfs_use[:,:-1]\n",
    "    iqrs = []\n",
    "    for i in range(n_gals_use):\n",
    "        modality.append(len(np.where(np.diff(np.signbit(dpdfs[i])))[0]))\n",
    "        cdf = np.cumsum(qp.utils.normalize_integral((dataset_info[dataset_key]['z_grid'], pdfs_use[i]), vb=False)[1])\n",
    "        iqr_lo = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.25)]\n",
    "        iqr_hi = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.75)]\n",
    "        iqrs.append(iqr_hi - iqr_lo)\n",
    "    modality = np.array(modality)\n",
    "        \n",
    "    dataset_info[dataset_key]['N_GMM'] = int(np.median(modality))+1\n",
    "#     print('n_gmm for '+dataset_info[dataset_key]['name']+' = '+str(dataset_info[dataset_key]['N_GMM']))\n",
    "      \n",
    "    # using the same grid for output as the native format, but doesn't need to be so\n",
    "    dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "    \n",
    "    print('preprocessed data in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = dataset_info[dataset_key]['in_z_grid']\n",
    "        info['pdfs'] = pdfs_use\n",
    "        info['modes'] = modality\n",
    "        info['iqrs'] = iqrs\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(pdfs_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(n_gals_use, dataset_key, bonus=None, norm=False):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        randos = info['randos']\n",
    "        z_grid = info['z_grid']\n",
    "        pdfs = info['pdfs']\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(n_plot):\n",
    "        data = (z_grid, pdfs[randos[i]])\n",
    "        data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "        pz_max.append(np.max(data))\n",
    "        plt.plot(data[0], data[1], label=dataset_info[dataset_key]['name']+' \\#'+str(randos[i]), color=color_cycle[i])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.title(dataset_info[dataset_key]['name']+' data examples', fontsize=16)\n",
    "    if norm:\n",
    "        plt.ylim(0., max(pz_max))\n",
    "        plt.savefig(loc+'norm.pdf', dpi=250)\n",
    "    else:\n",
    "        plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    if 'modes' in info.keys():\n",
    "        modes = info['modes']\n",
    "        modes_max.append(np.max(modes))\n",
    "        plt.figure()\n",
    "        ax = plt.hist(modes, color='k', alpha=1./n_plot, histtype='stepfilled', bins=range(max(modes_max)+1))\n",
    "        plt.xlabel('modes')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data modality distribution (median='+str(dataset_info[dataset_key]['N_GMM'])+')', fontsize=16)\n",
    "        plt.savefig(loc+'modality.pdf', dpi=250)\n",
    "        plt.close()\n",
    "        \n",
    "    if 'iqrs' in info.keys():\n",
    "        iqrs = info['iqrs']\n",
    "        iqr_min.append(min(iqrs))\n",
    "        iqr_max.append(max(iqrs))\n",
    "        plot_bins = np.linspace(min(iqr_min), max(iqr_max), 20)\n",
    "        plt.figure()\n",
    "        ax = plt.hist(iqrs, bins=plot_bins, color='k', alpha=1./n_plot, histtype='stepfilled')\n",
    "        plt.xlabel('IQR')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.title(dataset_info[dataset_key]['name']+' data IQR distribution', fontsize=16)\n",
    "        plt.savefig(loc+'iqrs.pdf', dpi=250)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to incrementally save the quantities that are costly to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_one_stat(dataset_name, n_gals_use, N_f, i, stat, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        hickle.dump(stat, filename)\n",
    "        \n",
    "def load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+str(N_f)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        stat = hickle.load(filename)\n",
    "#     print(stat)\n",
    "    return stat\n",
    "\n",
    "def save_moments_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_moments(dataset_name, n_gals_use, N_f, stat, stat_name)\n",
    "        \n",
    "def save_metrics_wrapper(dataset_name, n_gals_use, N_f, i, stat_name):\n",
    "    stat = load_one_stat(dataset_name, n_gals_use, N_f, i, stat_name)\n",
    "    save_nz_metrics(dataset_name, n_gals_use, N_f, stat, stat_name)\n",
    "    \n",
    "def clear_stats(dataset_name, n_gals_use, stat_name):\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name+'.hkl')\n",
    "    if os.path.isfile(loc):\n",
    "        os.remove(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_from_grid(dataset_key, in_pdfs, z_grid, N_comps, high_res=1000, bonus=None):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs in '+str(timeit.default_timer() - start))    \n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    start = timeit.default_timer()\n",
    "#     print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made a new ensemble from samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "#     print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    start = timeit.default_timer()\n",
    "#     print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, limits=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "    print('made the final ensemble in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['randos'] = randos\n",
    "        info['z_grid'] = z_grid\n",
    "        info['pdfs'] = Ef.evaluate(z_grid, using='truth', norm=True, vb=False)[1]\n",
    "        hickle.dump(info, filename)\n",
    "        \n",
    "    start = timeit.default_timer()\n",
    "#     print('calculating '+str(n_moments_use)+' moments of original PDFs')\n",
    "    in_moments, vals = [], []\n",
    "    for n in range(n_moments_use):\n",
    "        in_moments.append(Ef.moment(n, using='truth', limits=zlim, \n",
    "                                    dx=delta_z, vb=False))\n",
    "        vals.append(n)\n",
    "    moments = np.array(in_moments)\n",
    "    print('calculated '+str(n_moments_use)+' moments of original PDFs in '+str(timeit.default_timer() - start))\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(N_pdfs))\n",
    "    loc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['truth'] = moments\n",
    "        info['orders'] = vals\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(Ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, dataset_key, N_moments=4, i=None, bonus=None):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    qstart = timeit.default_timer()\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=True)\n",
    "    print('finished quantization in '+str(timeit.default_timer() - qstart))\n",
    "    hstart = timeit.default_timer()\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('finished histogramization in '+str(timeit.default_timer() - hstart))\n",
    "    sstart = timeit.default_timer()\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "    print('finished sampling in '+str(timeit.default_timer() - sstart))\n",
    "        \n",
    "    Eo = {}\n",
    "    \n",
    "    metric_start = timeit.default_timer()\n",
    "    inloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+bonus)\n",
    "    with open(inloc+'.hkl', 'r') as infilename:\n",
    "        pz_moments = hickle.load(infilename)\n",
    "    \n",
    "    klds, metrics, kld_moments, pz_moment_deltas = {}, {}, {}, {}\n",
    "    \n",
    "    for f in formats:\n",
    "        fstart = timeit.default_timer()\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'], \n",
    "                            limits=dataset_info[dataset_key]['z_lim'])\n",
    "        \n",
    "        fbonus = str(N_floats)+f+str(i)\n",
    "        loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+fbonus)\n",
    "        with open(loc+'.hkl', 'w') as filename:\n",
    "            info = {}\n",
    "            info['randos'] = randos\n",
    "            info['z_grid'] = z_grid\n",
    "            info['pdfs'] = Eo[f].evaluate(z_grid, using=f, norm=True, vb=False)[1]\n",
    "            hickle.dump(info, filename)\n",
    "        print('made '+f+' ensemble in '+str(timeit.default_timer()-fstart))\n",
    "\n",
    "        key = f\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z, vb=False)\n",
    "        print('calculated the '+key+' individual klds in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        fstart = timeit.default_timer()\n",
    "        kld_moments[key] = []\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples', vb=False)\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        for n in range(N_moments):\n",
    "            kld_moments[key].append(qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using='truth', \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        save_one_stat(name, size, n_floats_use, i, kld_moments, 'pz_kld_moments')\n",
    "        print('calculated the '+key+' kld moments in '+str(timeit.default_timer() - fstart))\n",
    "        \n",
    "        pz_moment_deltas[key], pz_moments[key] = [], []\n",
    "        for n in range(N_moments):\n",
    "            start = timeit.default_timer()\n",
    "            new_moment = Eo[key].moment(n, using=key, limits=zlim, \n",
    "                                                  dx=delta_z, vb=False)\n",
    "            pz_moments[key].append(new_moment)\n",
    "            #NOTE: delta_moment is crazy for clean data!\n",
    "            delta_moment = (new_moment - pz_moments['truth'][n]) / pz_moments['truth'][n]\n",
    "            pz_moment_deltas[key].append(delta_moment)\n",
    "            print('calculated the '+key+' individual moment '+str(n)+' in '+str(timeit.default_timer() - start))\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moments, 'pz_moments')\n",
    "        save_one_stat(name, size, n_floats_use, i, pz_moment_deltas, 'pz_moment_deltas')\n",
    "        \n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['N_floats'] = N_floats\n",
    "        info['pz_klds'] = klds\n",
    "        hickle.dump(info, filename)\n",
    "\n",
    "    outloc = os.path.join(path, 'pz_moments'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(outloc+'.hkl', 'w') as outfilename:\n",
    "        hickle.dump(pz_moments, outfilename)\n",
    "    \n",
    "#     save_moments(name, size, n_floats_use, kld_moments, 'pz_kld_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moments, 'pz_moments')\n",
    "#     save_moments(name, size, n_floats_use, pz_moment_deltas, 'pz_moment_deltas')\n",
    "    \n",
    "    return(Eo)#, klds, kld_moments, pz_moments, pz_moment_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_examples(name, size, N_floats, init, bonus={}):\n",
    "    path = os.path.join(name, str(size))\n",
    "    fig, ax = plt.subplots()\n",
    "#     fig_check, ax_check = plt.subplots()\n",
    "    lines = []\n",
    "    loc = os.path.join(path, 'pzs'+str(size)+name+'_postfit'+str(init))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        ref_pdfs = info['pdfs']  \n",
    "#     klds = {}\n",
    "    for bonus_key in bonus.keys():\n",
    "        loc = os.path.join(path, 'pzs'+str(size)+name+bonus_key)\n",
    "        with open(loc+'.hkl', 'r') as filename:\n",
    "            info = hickle.load(filename)\n",
    "            randos = info['randos']\n",
    "            z_grid = info['z_grid']\n",
    "            pdfs = info['pdfs']\n",
    "        ls = bonus[bonus_key][0]\n",
    "        a = bonus[bonus_key][1]\n",
    "        lab = re.sub(r'[\\_]', '', bonus_key)\n",
    "        line, = ax.plot([-1., 0.], [0., 0.], linestyle=ls, alpha=a, color='k', label=lab[:-1])\n",
    "        lines.append(line)\n",
    "        leg = ax.legend(loc='upper right', handles=lines)\n",
    "#         klds[bonus_key] = []\n",
    "        for i in range(n_plot):\n",
    "            data = (z_grid, pdfs[randos[i]])\n",
    "            data = qp.utils.normalize_integral(qp.utils.normalize_gridded(data))\n",
    "            ax.plot(data[0], data[1], linestyle=ls, alpha=a, color=color_cycle[i])\n",
    "            #     ax.legend(loc='upper right')\n",
    "#         for i in range(size):\n",
    "#             data = (z_grid, pdfs[i])\n",
    "#             kld = qp.utils.quick_kl_divergence(ref_pdfs[i], pdfs[i], dx=0.01)\n",
    "#             klds[bonus_key].append(kld)\n",
    "#     plot_bins = np.linspace(-3., 3., 20)\n",
    "#     for bonus_key in bonus.keys()[1:-1]:\n",
    "#         ax_check.hist(np.log(np.array(klds[bonus_key])), alpha=a, \n",
    "#                       histtype='stepfilled', edgecolor='k', \n",
    "#                       label=bonus_key, normed=True, bins=plot_bins, lw=2)\n",
    "    ax.set_xlabel(r'$z$', fontsize=14)\n",
    "    ax.set_ylabel(r'$p(z)$', fontsize=14)\n",
    "    ax.set_xlim(min(z_grid), max(z_grid))\n",
    "    ax.set_title(dataset_info[name]['name']+r' examples with $N_{f}=$'+str(N_floats), fontsize=16)\n",
    "    saveloc = os.path.join(path, 'pzs'+str(size)+name+str(N_floats)+'all'+str(init))\n",
    "    fig.savefig(saveloc+'.pdf', dpi=250)\n",
    "#     ax_check.legend()\n",
    "#     ax_check.set_ylabel('frequency', fontsize=14)\n",
    "#     ax_check.set_xlabel(r'$\\mathrm{KLD}$', fontsize=14)\n",
    "#     ax_check.set_title(name+r' data $p(\\mathrm{KLD})$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "#     fig_check.savefig(saveloc+'kld_check.pdf', dpi=250)\n",
    "    plt.close()\n",
    "#     with open(saveloc+'.p', 'w') as kldfile:\n",
    "#         pickle.dump(klds, kldfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_kld(n_gals_use, dataset_key, N_floats, i):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    a = 1./len(formats)\n",
    "    loc = os.path.join(path, 'kld_hist'+str(n_gals_use)+dataset_key+str(N_floats)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        N_floats = info['N_floats']\n",
    "        pz_klds = info['pz_klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_bins = np.linspace(-10., 5., 30)\n",
    "    for key in pz_klds.keys():\n",
    "        logdata = qp.utils.safelog(pz_klds[key])\n",
    "        dist_min.append(min(logdata))\n",
    "        dist_max.append(max(logdata))\n",
    "#         plot_bins = np.linspace(-10., 5., 20)\n",
    "        kld_hist = plt.hist(logdata, color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "             label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=2)\n",
    "#         kld_hist = plt.hist(pz_klds[key], color=colors[key], alpha=a, histtype='stepfilled', edgecolor='k',\n",
    "#              label=key, normed=True, bins=plot_bins, linestyle=stepstyles[key], ls=stepstyles[key], lw=2)\n",
    "        hist_max.append(max(kld_hist[0]))\n",
    "#     print(loc+': min log[KLD]='+str(logdata)+' at N='+str(np.argmin(logdata)))\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency', fontsize=14)\n",
    "#     plt.xlabel(r'$\\log[\\mathrm{KLD}]$', fontsize=14)\n",
    "    plt.xlabel(r'$\\log[\\mathrm{KLD}]$', fontsize=14)\n",
    "#     plt.xlim(min(dist_min), max(dist_max))\n",
    "    plt.ylim(0., max(hist_max))\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $p(\\log[\\mathrm{KLD}])$ with $N_{f}='+str(N_floats)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_kld(size, name, i):\n",
    "    path = os.path.join(name, str(size))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.canvas.draw()\n",
    "    for i in instantiations:\n",
    "        to_plot = {}\n",
    "        for f in formats:\n",
    "            to_plot[f] = []\n",
    "        for Nf in floats:\n",
    "            place = os.path.join(path, 'kld_hist'+str(size)+name+str(Nf)+'_'+str(i))\n",
    "            with open(place+'.hkl', 'r') as filename:\n",
    "                klds = hickle.load(filename)['pz_klds']\n",
    "                for f in formats:\n",
    "                    to_plot[f].append(klds[f])\n",
    "#                         print(name, size, i, Nf, f, klds[f])\n",
    "        for f in formats:\n",
    "            to_plot[f] = np.array(to_plot[f])\n",
    "            delta_info = np.ones((len(floats), size))\n",
    "            for Nf in floats:\n",
    "                delta_info[:-1] = to_plot[f][1:] - to_plot[f][:-1]\n",
    "                delta_info[-1] = -1. * to_plot[f][-1]\n",
    "            ax.plot(floats, delta_info, color=colors[f])\n",
    "        ax.set_xlabel()\n",
    "        ax.set_ylabel()\n",
    "        ax.semilogx()\n",
    "        ax.set_xticks(floats)\n",
    "        ax.set_xticklabels([r'$3\\to 10$', r'$10\\to 30$', r'$30\\to 100$', r'$100\\to \\infty$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid, n_floats_use, dataset_key, i=None):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    n_gals_use = E0.n_pdfs\n",
    "    \n",
    "#     print('stacking the ensembles')\n",
    "#     stack_start = timeit.default_timer()\n",
    "    stacked_pdfs, stacks = {}, {}\n",
    "    for key in formats:\n",
    "        start = timeit.default_timer()\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "        stacks[key] = stacked_pdfs[key].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "        print('stacked '+key+ ' in '+str(timeit.default_timer()-start))\n",
    "    \n",
    "    stack_start = timeit.default_timer()\n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    \n",
    "    stacks['truth'] = stacked_pdfs['truth'].evaluate(z_grid, using='gridded', norm=True, vb=False)[1]\n",
    "    print('stacked truth in '+str(timeit.default_timer() - stack_start))\n",
    "    \n",
    "    klds = {}\n",
    "    for key in formats:\n",
    "        kld_start = timeit.default_timer()\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "        print('calculated the '+key+' stacked kld in '+str(timeit.default_timer() - kld_start))\n",
    "    save_one_stat(dataset_key, n_gals_use, n_floats_use, i, klds, 'nz_klds')\n",
    "#     save_nz_metrics(name, size, n_floats_use, klds, 'nz_klds')\n",
    "        \n",
    "    moments = {}\n",
    "    for key in formats_plus:\n",
    "        moment_start = timeit.default_timer()\n",
    "        moments[key] = []\n",
    "        for n in range(n_moments_use):\n",
    "            moments[key].append(qp.utils.calculate_moment(stacked_pdfs[key], n, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False))\n",
    "        print('calculated the '+key+' stacked moments in '+str(timeit.default_timer() - moment_start))\n",
    "    save_one_stat(dataset_key, n_gals_use, n_floats_use, i, moments, 'nz_moments')\n",
    "#     save_moments(name, size, n_floats_use, moments, 'nz_moments') \n",
    "    \n",
    "    path = os.path.join(dataset_key, str(E0.n_pdfs))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'w') as filename:\n",
    "        info = {}\n",
    "        info['z_grid'] = z_grid\n",
    "        info['stacks'] = stacks\n",
    "        info['klds'] = klds\n",
    "        info['moments'] = moments\n",
    "        hickle.dump(info, filename)\n",
    "    \n",
    "    return(stacked_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimators(n_gals_use, dataset_key, n_floats_use, i=None):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "    with open(loc+'.hkl', 'r') as filename:\n",
    "        info = hickle.load(filename)\n",
    "        z_grid = info['z_grid']\n",
    "        stacks = info['stacks']\n",
    "        klds = info['klds']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(z_grid, stacks['truth'], color='black', lw=3, alpha=0.3, label='original')\n",
    "    nz_max.append(max(stacks['truth']))\n",
    "    for key in formats:\n",
    "        nz_max.append(max(stacks[key]))\n",
    "        plt.plot(z_grid, stacks[key], label=key+r' KLD='+str(klds[key])[:8], color=colors[key], linestyle=styles[key])\n",
    "    plt.xlabel(r'$z$', fontsize=14)\n",
    "    plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "    plt.xlim(min(z_grid), max(z_grid))\n",
    "    plt.ylim(0., max(nz_max))\n",
    "    plt.legend()\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "    plt.savefig(loc+'.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_moments(dataset_name, n_gals_use, N_f, stat, stat_name):\n",
    "\n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name)\n",
    "    \n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as stat_file:\n",
    "        #read in content of list/dict\n",
    "            stats = hickle.load(stat_file)\n",
    "    else:\n",
    "        stats = {}\n",
    "        stats['N_f'] = []\n",
    "        for f in stat.keys():\n",
    "            stats[f] = []\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f].append([])\n",
    "\n",
    "    if N_f not in stats['N_f']:\n",
    "        stats['N_f'].append(N_f)\n",
    "        for f in stat.keys():\n",
    "            for m in range(n_moments_use):\n",
    "                stats[f][m].append([])\n",
    "        \n",
    "    where_N_f = stats['N_f'].index(N_f)\n",
    "        \n",
    "    for f in stat.keys():\n",
    "        for m in range(n_moments_use):\n",
    "            stats[f][m][where_N_f].append(stat[f][m])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as stat_file:\n",
    "        hickle.dump(stats, stat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include second axis with mean KLD values?\n",
    "# somehow combining pz_kld_moments with this?\n",
    "# something is not right here with limits, need to check after nersc run\n",
    "def plot_kld_stats(name, size):\n",
    "    a = 1./len(formats)\n",
    "    topdir = os.path.join(name, str(size))\n",
    "    \n",
    "    fig_one, ax_one = plt.subplots(figsize=(5, 5))\n",
    "    fig_one.canvas.draw()\n",
    "    mean_deltas, std_deltas = {}, {}\n",
    "    for f in formats:\n",
    "        mean_deltas[f], std_deltas[f] = [], []\n",
    "        ax_one.plot([1000., 1000.], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "    for i in instantiations:\n",
    "        to_plot = {}\n",
    "        for f in formats:\n",
    "            to_plot[f] = []\n",
    "            mean_deltas[f].append([])\n",
    "            std_deltas[f].append([])\n",
    "        for Nf in floats:\n",
    "            loc = os.path.join(topdir, 'kld_hist'+str(size)+name+str(Nf)+'_'+str(i))\n",
    "            with open(loc+'.hkl', 'r') as filename:\n",
    "                klds = hickle.load(filename)['pz_klds']\n",
    "                for f in formats:\n",
    "                    to_plot[f].append(klds[f])\n",
    "        for f in formats:\n",
    "            to_plot[f] = np.array(to_plot[f])\n",
    "            delta_info = np.zeros((len(floats), size))\n",
    "            delta_info[:-1] = to_plot[f][:-1] - to_plot[f][1:]\n",
    "            delta_info[-1] = to_plot[f][-1]\n",
    "#             delta_info[delta_info < qp.utils.epsilon] = qp.utils.epsilon\n",
    "#             log_delta_info = np.log(delta_info)\n",
    "#             ax_one.plot(floats, log_delta_info)\n",
    "            mean_deltas[f][i] = np.mean(delta_info, axis=1)\n",
    "            std_deltas[f][i] = np.std(delta_info, axis=1)\n",
    "            indie_delta_kld_min.append(np.min(mean_deltas[f][i] - std_deltas[f][i]))\n",
    "            indie_delta_kld_max.append(np.max(mean_deltas[f][i] + std_deltas[f][i]))\n",
    "            ax_one.plot(floats, mean_deltas[f][i], color=colors[f], alpha=a, linestyle=styles[f])\n",
    "    ax_one.set_ylabel(r'$\\Delta\\mathrm{KLD}$ (nats)')\n",
    "#     ax_one.semilogy()\n",
    "    ax_one.set_ylim(0., np.max(indie_delta_kld_max))\n",
    "    ax_one.set_xlim(min(floats), max(floats))\n",
    "    ax_one.set_xlabel('change in number of parameters')\n",
    "    ax_one.semilogx()\n",
    "    ax_one.set_xticks(floats)\n",
    "    ax_one.set_xticklabels([r'$3\\to 10$', r'$10\\to 30$', r'$30\\to 100$', r'$100\\to \\infty$'])\n",
    "    ax_one.legend(loc='upper right')\n",
    "    ax_one.set_title(dataset_info[name]['name']+r' data per-PDF $\\Delta\\mathrm{KLD}$', fontsize=16)\n",
    "    place = os.path.join(topdir, 'indie_klds'+str(size)+name)\n",
    "    fig_one.savefig(place+'_each.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    for f in formats:\n",
    "        mean_deltas[f] = np.array(mean_deltas[f])\n",
    "        std_deltas[f] = np.array(std_deltas[f])\n",
    "        global_delta_mean = np.mean(mean_deltas[f], axis=0)\n",
    "        global_delta_std = np.sqrt(np.sum(mean_deltas[f]**2, axis=0))\n",
    "        print(global_delta_mean, global_delta_std)\n",
    "#         x_cor = np.array([floats[:-1], floats[:-1], floats[1:], floats[1:]])\n",
    "        y_plus = global_delta_mean + global_delta_std\n",
    "        y_minus = global_delta_mean - global_delta_std\n",
    "#         y_minus[y_minus < qp.utils.epsilon] = qp.utils.epsilon\n",
    "        indie_delta_kld_min.append(np.min(y_minus))\n",
    "        indie_delta_kld_max.append(np.max(y_plus))\n",
    "#         y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "#         ax.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "        ax.fill_between(floats, y_minus, y_plus, color=colors[f], alpha=a, linewidth=0.)\n",
    "        ax.plot(floats, global_delta_mean, color=colors[f], linestyle=styles[f], label=f)\n",
    "    ax.set_ylabel(r'$\\Delta\\mathrm{KLD}$ (nats)')\n",
    "#     ax.semilogy()\n",
    "    ax.set_ylim(0., np.max(indie_delta_kld_max))\n",
    "    ax.set_xlim(min(floats), max(floats))\n",
    "    ax.set_xlabel('change in number of parameters')\n",
    "    ax.semilogx()\n",
    "    ax.set_xticks(floats)\n",
    "    ax.set_xticklabels([r'$3\\to 10$', r'$10\\to 30$', r'$30\\to 100$', r'$100\\to \\infty$'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(dataset_info[name]['name']+r' data per-PDF $\\Delta\\mathrm{KLD}$', fontsize=16)\n",
    "    place = os.path.join(topdir, 'indie_klds'+str(size)+name)\n",
    "    fig.savefig(place+'_clean.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pz_metrics(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'pz_kld_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "  \n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "\n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)\n",
    "\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    a = 1./len(formats)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-1], [0], color=colors[key], label=key, linewidth=2, linestyle=styles[key], alpha=0.5)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-1], [0], color='k', alpha=0.5, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            mean = np.mean(data_arr, axis=0).flatten()\n",
    "            std = np.std(data_arr, axis=0).flatten()\n",
    "            y_plus = mean + std\n",
    "            y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, marker=shapes[n], mfc='none', markersize=marksize, linestyle=styles[f], alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            pz_mean_max[n] = max(pz_mean_max[n], np.max(y_plus))\n",
    "            pz_mean_min[n] = min(pz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim((pz_mean_min[n]-1., pz_mean_max[n]+1.))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[\\mathrm{KLD}]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-1], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-1], [0], color='k', alpha=0.5, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "#             print('pz metrics data shape '+str(pz_stats[f][n]))\n",
    "            data_arr = np.log(np.swapaxes(np.array(pz_stats[f][n]), 0, 1))#go from n_floats*instantiations to instantiations*n_floats\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "#                 pz_moment_max[n-1].append(max(i))\n",
    "        ax_n.set_ylabel(r'$\\log[\\mathrm{'+moment_names[n]+r'}]$', rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(pz_mean_min[n]-1., pz_mean_max[n]+1.)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\log[\\mathrm{KLD}]$ log-moments', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pz_delta_moments(name, size):\n",
    "    n_gals_use = size\n",
    "    extremum = np.zeros(n_moments_use)\n",
    "    \n",
    "    # should look like nz_moments\n",
    "    path = os.path.join(name, str(size))\n",
    "    loc = os.path.join(path, 'pz_moment_deltas'+str(size)+name)\n",
    "    with open(loc+'.hkl', 'r') as pz_file:\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "    flat_floats = np.array(pz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=0.5, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            old_shape = np.shape(np.array(pz_stats[f][n]))\n",
    "            new_shape = (old_shape[0], np.prod(old_shape[1:]))\n",
    "            data_arr = np.abs(np.array(pz_stats[f][n]).reshape(new_shape)) * 100.#go from n_floats*instantiations*n_gals n_floats*(n_gals*n_instantiations)\n",
    "#             data_arr = np.median(data_arr, axis=2) * 100.\n",
    "#             data_arr = np.swapaxes(np.array(nz_stats[f][n]), 0, 1)* 100.#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "#             mean = np.mean(data_arr, axis=0).flatten()\n",
    "#             std = np.std(data_arr, axis=0).flatten()\n",
    "#             mean = np.median(data_arr, axis=-1)\n",
    "            std = np.log10(np.percentile(data_arr, [25, 50, 75], axis=-1))\n",
    "            y_plus = std[-1]#mean + std\n",
    "            y_minus = std[0]#mean - std\n",
    "            mean = std[1]\n",
    "#             y_cor = np.array([y_minus, y_plus, y_plus, y_minus])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "#             print('before '+str((np.shape(data_arr), n, n_delta_max, n_delta_min, y_plus, y_minus)))\n",
    "            n_delta_max[n] = max(n_delta_max[n], np.max(y_plus))\n",
    "            n_delta_min[n] = min(n_delta_min[n], np.min(y_minus))\n",
    "#             old_shape = np.shape(np.array(pz_stats[f][n]))\n",
    "#             new_shape = (old_shape[0], np.prod(old_shape[1:]))\n",
    "#             data_arr = np.array(pz_stats[f][n]).reshape(new_shape)#go from n_floats*instantiations to instantiations*n_floats\n",
    "# #             data_arr = np.array(pz_stats[f][n])\n",
    "# #             data_arr = np.median(data_arr, axis=2) * 100.\n",
    "#             mean = np.mean(data_arr, axis=1)\n",
    "#             std = np.std(data_arr, axis=1)\n",
    "#             y_plus = (mean + std) * 100.\n",
    "#             y_minus = (mean - std) * 100.\n",
    "# #             y_cor = np.array([y_minus, y_plus, y_plus, y_minus])\n",
    "#             ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, alpha=a, color=colors[f])\n",
    "#             ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "#             print('before '+str((np.shape(data_arr), n, n_delta_max, n_delta_min, y_plus, y_minus)))\n",
    "#             n_delta_max[n] = np.max(n_delta_max[n], np.max(y_plus))\n",
    "#             n_delta_min[n] = np.min(n_delta_min[n], np.min(y_minus))\n",
    "#             print('after '+str((n_delta_max, n_delta_min)))\n",
    "        ax_n.set_ylabel(r'$\\log_{10}$-percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        extremum[n] = np.max(np.abs(np.array([n_delta_min[n], n_delta_max[n]]))) + 0.25\n",
    "        ax_n.set_ylim(-1.*extremum[n], extremum[n])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{p}(z)$ moment log-percent errors', fontsize=16)\n",
    "    ax.legend(loc=dataset_info[name]['legloc_p'])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=a, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = np.swapaxes(np.array(pz_stats[f][n]), 0, 1)\n",
    "            data_arr = np.median(data_arr, axis=2) * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'median percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(-10., 10.)#(-1.*extremum[n], extremum[n])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[name]['name']+r' data $\\hat{p}(z)$ moment percent errors', fontsize=16)\n",
    "    ax.legend(loc='upper left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nz_metrics(dataset_name, n_gals_use, N_f, nz_klds, stat_name):\n",
    "    \n",
    "    path = os.path.join(dataset_name, str(n_gals_use))\n",
    "    loc = os.path.join(path, stat_name+str(n_gals_use)+dataset_name)\n",
    "    if os.path.exists(loc+'.hkl'):\n",
    "        with open(loc+'.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "            nz_stats = hickle.load(nz_file)\n",
    "    else:\n",
    "        nz_stats = {}\n",
    "        nz_stats['N_f'] = []\n",
    "        for f in formats:\n",
    "            nz_stats[f] = []\n",
    "    \n",
    "    if N_f not in nz_stats['N_f']:\n",
    "        nz_stats['N_f'].append(N_f)\n",
    "        for f in formats:\n",
    "            nz_stats[f].append([])\n",
    "        \n",
    "    where_N_f = nz_stats['N_f'].index(N_f) \n",
    "    \n",
    "    for f in formats:\n",
    "        nz_stats[f][where_N_f].append(nz_klds[f])\n",
    "\n",
    "    with open(loc+'.hkl', 'w') as nz_file:\n",
    "        hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nz_klds(dataset_key, n_gals_use):\n",
    "    \n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_klds'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "#     if len(instantiations) == 10:\n",
    "#         for f in formats:\n",
    "#             if not np.shape(nz_stats[f]) == (4, 10):\n",
    "#                 for s in range(len(floats)):\n",
    "#                     nz_stats[f][s] = np.array(np.array(nz_stats[f][s])[:10]).flatten()\n",
    "\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./len(formats)#1./n_i\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], alpha=a, label=f, linestyle=styles[f])\n",
    "        for i in data_arr:\n",
    "            plt.plot(flat_floats, i, color=colors[f], alpha=a, linestyle=styles[f])\n",
    "            kld_min.append(min(i))\n",
    "            kld_max.append(max(i))\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min), max(kld_max))\n",
    "    plt.xlim(min(flat_floats), max(flat_floats))\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_info[dataset_key]['name']+' mock catalog', fontsize=16)\n",
    "    plt.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    a = 1./len(formats)\n",
    "    for f in formats:\n",
    "#         print('nz klds data shape '+str(nz_stats[f][n]))\n",
    "        data_arr = np.swapaxes(np.array(nz_stats[f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "        plt.plot([10. * max(flat_floats), 10. * max(flat_floats)], [1., 10.], color=colors[f], label=f, linestyle=styles[f])\n",
    "        kld_min.append(np.min(data_arr))\n",
    "        kld_max.append(np.max(data_arr))\n",
    "        mean = np.mean(np.log(data_arr), axis=0)\n",
    "        std = np.std(np.log(data_arr), axis=0)\n",
    "        x_cor = np.array([flat_floats[:-1], flat_floats[:-1], flat_floats[1:], flat_floats[1:]])\n",
    "        y_plus = np.exp(mean + std)\n",
    "        y_minus = np.exp(mean - std)\n",
    "        y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "        plt.plot(flat_floats, np.exp(mean), color=colors[f], linestyle=styles[f])\n",
    "        plt.fill(x_cor, y_cor, color=colors[f], alpha=a, linewidth=0.)\n",
    "    plt.semilogy()\n",
    "    plt.semilogx()\n",
    "    plt.xticks(flat_floats, [str(ff) for ff in flat_floats])\n",
    "    plt.ylim(min(kld_min), max(kld_max))\n",
    "    plt.xlim(min(flat_floats), max(flat_floats))\n",
    "    plt.xlabel(r'number of parameters', fontsize=14)\n",
    "    plt.ylabel(r'KLD', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ KLD', fontsize=16)\n",
    "    plt.savefig(loc+'_clean.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nz_moments(dataset_key, n_gals_use):\n",
    "\n",
    "    path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    loc = os.path.join(path, 'nz_moments'+str(n_gals_use)+dataset_key)\n",
    "    with open(loc+'.hkl', 'r') as nz_file:\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "    flat_floats = np.array(nz_stats['N_f']).flatten()\n",
    "    in_x = np.log(flat_floats)\n",
    "    a = 1./len(formats)\n",
    "    shapes = moment_shapes\n",
    "    marksize = 10\n",
    "    \n",
    "    def make_patch_spines_invisible(ax):\n",
    "        ax.set_frame_on(True)\n",
    "        ax.patch.set_visible(False)\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_visible(False)   \n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "    for n in range(1, n_moments_use):\n",
    "        ax.scatter([-10], [0], color='k', alpha=0.5, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "#             data_arr = np.abs(np.array(pz_stats[f][n]).reshape(new_shape)) * 100.#go from n_floats*instantiations*n_gals n_floats*(n_gals*n_instantiations)\n",
    "#             data_arr = np.median(data_arr, axis=2) * 100.\n",
    "#             data_arr = np.swapaxes(np.array(nz_stats[f][n]), 0, 1)* 100.#np.log(np.swapaxes(np.array(nz_stats[f]), 0, 1)[:][:][n])#go from n_floats*instantiations to instantiations*n_floats\n",
    "#             mean = np.mean(data_arr, axis=0).flatten()\n",
    "#             std = np.std(data_arr, axis=0).flatten()\n",
    "#             mean = np.median(data_arr, axis=-1)\n",
    "#             std = np.log10(np.percentile(np.abs(data_arr), [25, 50, 75], axis=0))\n",
    "            std = np.percentile(data_arr, [25, 50, 75], axis=0)\n",
    "            y_plus = std[-1]#mean + std\n",
    "            y_minus = std[0]#mean - std\n",
    "            mean = std[1]\n",
    "#             mean = np.mean(data_arr, axis=0).flatten()\n",
    "#             std = np.std(data_arr, axis=0).flatten()\n",
    "#             y_plus = mean + std\n",
    "#             y_minus = mean - std\n",
    "#             y_cor = np.array([y_minus[:-1], y_plus[:-1], y_plus[1:], y_minus[1:]])\n",
    "            ax_n.plot(np.exp(in_x+n_factor), mean, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, alpha=a, color=colors[f])\n",
    "            ax_n.vlines(np.exp(in_x+n_factor), y_minus, y_plus, linewidth=3., alpha=a, color=colors[f])\n",
    "            nz_mean_max[n] = max(nz_mean_max[n], np.max(y_plus))\n",
    "            nz_mean_min[n] = min(nz_mean_min[n], np.min(y_minus))\n",
    "        ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        extremum = np.max(np.abs([nz_mean_min[n], nz_mean_max[n]])) + 1.#0.25\n",
    "        ax_n.set_ylim(-1. * extremum, extremum)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moment percent errors', fontsize=16)\n",
    "    ax.legend(loc=dataset_info[name]['legloc_n'])#FINDME!\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_clean_unlog.pdf', dpi=250)\n",
    "    plt.close()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    ax_n = ax\n",
    "    for key in formats:\n",
    "        ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "    for n in range(1, n_moments_use):\n",
    "        n_factor = 0.1 * (n - 2)\n",
    "        ax.scatter([-10], [0], color='k', alpha=0.5, marker=shapes[n], s=50, label=moment_names[n])\n",
    "        truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "        if n>1:\n",
    "            ax_n = ax.twinx()\n",
    "            rot_ang = 270\n",
    "            label_space = 15.\n",
    "        else:\n",
    "            rot_ang = 90\n",
    "            label_space = 0.\n",
    "        if n>2:\n",
    "            ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "            make_patch_spines_invisible(ax_n)\n",
    "            ax_n.spines[\"right\"].set_visible(True)\n",
    "        for s in range(len(formats)):\n",
    "            f = formats[s]\n",
    "            f_factor = 0.05 * (s - 1)\n",
    "            data_arr = (np.swapaxes(np.array(nz_stats[f][n]), 0, 1) - truth) / truth * 100.\n",
    "            for i in data_arr:\n",
    "                ax_n.plot(np.exp(in_x+n_factor), i, linestyle=styles[f], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=a)\n",
    "        ax_n.set_ylabel(r'median percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "        ax_n.set_ylim(-15., 15.)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(flat_floats)\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "    ax.set_xlabel('number of parameters', fontsize=14)\n",
    "    ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moment percent errors', fontsize=16)\n",
    "    ax.legend(loc='lower left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(loc+'_all.pdf', dpi=250)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_nz_moments(dataset_key, n_gals_use):\n",
    "#     path = os.path.join(dataset_key, str(n_gals_use))\n",
    "    \n",
    "#     dz = dataset_info[dataset_key]['delta_z']\n",
    "#     z_grid = dataset_info[dataset_key]['z_grid']\n",
    "#     full_stack = {}\n",
    "#     all_moments = {}\n",
    "#     for f in formats_plus:\n",
    "#         full_stack[f] = []\n",
    "#         all_moments[f] = []\n",
    "#     for nf in range(len(floats)):\n",
    "#         n_floats_use = floats[nf]\n",
    "#         for f in formats_plus:\n",
    "#             full_stack[f].append(np.zeros(len(z_grid)))\n",
    "#             all_moments[f].append([])\n",
    "#         for i in instantiations:\n",
    "#             loc = os.path.join(path, 'nz_comp'+str(n_gals_use)+dataset_key+str(n_floats_use)+'_'+str(i))\n",
    "#             with open(loc+'.hkl', 'r') as filename:\n",
    "#                 info = hickle.load(filename)\n",
    "# #                 z_grid = info['z_grid']\n",
    "#                 stacks = info['stacks']\n",
    "# #                 klds = info['klds']\n",
    "#             for key in formats_plus:\n",
    "#                 full_stack[key][nf] += stacks[key]\n",
    "#         for n in range(1, n_moments_use):\n",
    "#             ngrid = z_grid**n\n",
    "#             all_moments['truth'][nf].append(qp.utils.quick_moment(full_stack['truth'][nf], ngrid, dz))\n",
    "#             for key in formats:\n",
    "#                 all_moments[key][nf].append((qp.utils.quick_moment(full_stack[key][nf], ngrid, dz) - all_moments['truth'][nf][-1]) / all_moments['truth'][nf][-1])\n",
    "#     for f in formats:\n",
    "#         all_moments[f] = np.array(all_moments[f])\n",
    "#     print(dataset_key, n_gals_use, all_moments)\n",
    "          \n",
    "#     in_x = np.log(floats)\n",
    "#     a = 1./len(formats)\n",
    "#     shapes = moment_shapes\n",
    "#     marksize = 7.5\n",
    "    \n",
    "#     def make_patch_spines_invisible(ax):\n",
    "#         ax.set_frame_on(True)\n",
    "#         ax.patch.set_visible(False)\n",
    "#         for sp in ax.spines.values():\n",
    "#             sp.set_visible(False)\n",
    "        \n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.subplots_adjust(right=1.)\n",
    "#     ax_n = ax\n",
    "#     for key in formats:\n",
    "#         ax_n.plot([-10], [0], color=colors[key], label=key, linestyle=styles[key], alpha=0.5, linewidth=2)\n",
    "#     for n in range(1, n_moments_use):\n",
    "#         n_factor = 0.1 * (n - 2)\n",
    "#         ax.scatter([-10], [0], color='k', alpha=0.5, marker=shapes[n], facecolors='none', s=50, label=moment_names[n])\n",
    "# #         truth = np.swapaxes(np.array(nz_stats['truth'][n]), 0, 1)\n",
    "#         if n>1:\n",
    "#             ax_n = ax.twinx()\n",
    "#             rot_ang = 270\n",
    "#             label_space = 15.\n",
    "#         else:\n",
    "#             rot_ang = 90\n",
    "#             label_space = 0.\n",
    "#         if n>2:\n",
    "#             ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "#             make_patch_spines_invisible(ax_n)\n",
    "#             ax_n.spines[\"right\"].set_visible(True)\n",
    "#         for s in range(len(formats)):\n",
    "#             f = formats[s]\n",
    "#             f_factor = 0.05 * (s - 1)\n",
    "#             data_arr = np.swapaxes(all_moments[f], 0, 1) * 100.\n",
    "#             ax_n.plot(np.exp(in_x+n_factor), data_arr[n-1], linestyle=styles[f], color=colors[f], alpha=a)\n",
    "#             ax_n.scatter(np.exp(in_x+n_factor), data_arr[n-1], marker=shapes[n], mfc='none', markersize=marksize, color=colors[f], alpha=0.5)\n",
    "#         ax_n.set_ylabel(r'percent error on '+moment_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "# #         ax_n.set_ylim(-1. * extremum, extremum)\n",
    "#     ax.set_xscale('log')\n",
    "#     ax.set_xticks(floats)\n",
    "#     ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "#     ax.set_xlim(np.exp(min(in_x)-0.25), np.exp(max(in_x)+0.25))\n",
    "#     ax.set_xlabel('number of parameters', fontsize=14)\n",
    "#     ax.set_title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ moments', fontsize=16)\n",
    "#     ax.legend(loc='lower left')\n",
    "#     fig.tight_layout()\n",
    "#     outloc = os.path.join(path, 'nz_moments'+str(n_gals_use)+dataset_key)\n",
    "#     fig.savefig(outloc+'_final.pdf', dpi=250)\n",
    "#     plt.close()\n",
    "    \n",
    "#     for nf in range(len(floats)):\n",
    "#         n_floats_use = floats[nf]\n",
    "#         plt.figure()\n",
    "#         plt.plot(z_grid, full_stack['truth'][nf], color='black', lw=3, alpha=0.3, label='original')\n",
    "#         for key in formats:\n",
    "#             kld = qp.utils.quick_kl_divergence(full_stack['truth'][nf], full_stack[key][nf], dx=dz)\n",
    "#             plt.plot(z_grid, full_stack[key][nf], color=colors[key], linestyle=styles[key], label=key+r' KLD='+str(kld)[:8])#+r'; '+str(all_moments[f][nf])+' percent error')\n",
    "#         plt.xlabel(r'$z$', fontsize=14)\n",
    "#         plt.ylabel(r'$\\hat{n}(z)$', fontsize=14)\n",
    "#         plt.xlim(min(z_grid), max(z_grid))\n",
    "#     #     plt.ylim(0., max(nz_max))\n",
    "#         plt.legend()\n",
    "#         plt.title(dataset_info[dataset_key]['name']+r' data $\\hat{n}(z)$ with $N_{f}='+str(n_floats_use)+r'$', fontsize=16)\n",
    "#         outloc = os.path.join(path, 'global_nz'+str(n_gals_use)+dataset_key+str(n_floats_use))\n",
    "#         plt.savefig(outloc+'.pdf', dpi=250)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, number of floats, and instantiations!\n",
    "\n",
    "Note: It takes about 5 minutes per \\# floats considered for 100 galaxies, and about 40 minutes per \\# floats for 1000 galaxies.  (So, yes, it scales more or less as expected!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = {}\n",
    "delta = 0.01\n",
    "\n",
    "dataset_keys = ['mg', 'ss']\n",
    "\n",
    "for name in dataset_keys:\n",
    "    dataset_info[name] = {}\n",
    "    if name == 'mg':\n",
    "        datafilename = 'bpz_euclid_test_10_3.probs'\n",
    "        z_low = 0.01\n",
    "        z_high = 3.51\n",
    "        nc_needed = 3\n",
    "        plotname = 'brighter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "        legloc_p = 'upper right'\n",
    "        legloc_n = 'upper left'\n",
    "    elif name == 'ss':\n",
    "        datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "        z_low = 0.005\n",
    "        z_high = 2.11\n",
    "        nc_needed = 5\n",
    "        plotname = 'fainter'\n",
    "        skip_rows = 1\n",
    "        skip_cols = 1\n",
    "        legloc_p = 'lower left'\n",
    "        legloc_n = 'lower right'\n",
    "    dataset_info[name]['filename'] = datafilename  \n",
    "    \n",
    "    dataset_info[name]['z_lim'] = (z_low, z_high)\n",
    "    z_grid = np.arange(z_low, z_high, delta, dtype='float')#np.arange(z_low, z_high + delta, delta, dtype='float')\n",
    "    z_range = z_high - z_low\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    dataset_info[name]['z_grid'] = z_grid\n",
    "    dataset_info[name]['delta_z'] = delta_z\n",
    "\n",
    "    dataset_info[name]['N_GMM'] = nc_needed# will be overwritten later\n",
    "    dataset_info[name]['name'] = plotname\n",
    "    dataset_info[name]['legloc_p'] = legloc_p\n",
    "    dataset_info[name]['legloc_n'] = legloc_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "formats_plus = list(formats)\n",
    "formats_plus.append('truth')\n",
    "n_formats =len(formats)\n",
    "\n",
    "high_res = 300\n",
    "\n",
    "color_cycle = np.array([(230, 159, 0), (86, 180, 233), (0, 158, 115), (240, 228, 66), (0, 114, 178), (213, 94, 0), (204, 121, 167)])/256.\n",
    "color_cycle_names = ['Orange', 'Sky blue', 'Bluish green', 'Yellow', 'Blue', 'Vermilion', 'Reddish purple']\n",
    "n_plot = len(color_cycle)\n",
    "\n",
    "n_moments_use = 4\n",
    "n_symb = 5\n",
    "moment_names = ['integral', 'mean', 'variance', 'skewness']\n",
    "moment_shapes = [(n_symb, 3, 0), (n_symb, 0, 0), (n_symb, 1, 0), (n_symb, 2, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging, specify the randomly selected PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all for NERSC\n",
    "\n",
    "floats = [3, 10, 30, 100]\n",
    "sizes = [100]#[10, 100, 1000]\n",
    "names = dataset_info.keys()\n",
    "instantiations = range(0, 10)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) for size in sizes] for name in names]\n",
    "# all_randos = [[np.random.choice(indices, n_plot, replace=False) for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pipeline\" is a bunch of nested `for` loops because `qp.Ensemble` makes heavy use of multiprocessing.  Doing multiprocessing within multiprocessing may or may not cause problems, but I am certain that it makes debugging a nightmare.\n",
    "\n",
    "Okay, without further ado, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the \"pipeline\"\n",
    "# global_start = timeit.default_timer()\n",
    "# for n in range(len(names)):\n",
    "#     name = names[n]\n",
    "    \n",
    "#     dataset_start = timeit.default_timer()\n",
    "#     print('started '+name)\n",
    "    \n",
    "#     pdfs = setup_dataset(name, skip_rows, skip_cols)\n",
    "    \n",
    "#     for s in range(len(sizes)):\n",
    "#         size = sizes[s]\n",
    "        \n",
    "#         size_start = timeit.default_timer()\n",
    "#         print('started '+name+str(size))\n",
    "        \n",
    "#         path = os.path.join(name, str(size))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "        \n",
    "#         n_gals_use = size\n",
    "        \n",
    "#         randos = all_randos[n][s]\n",
    "        \n",
    "#         for i in instantiations:\n",
    "# #             top_bonusdict = {}\n",
    "#             i_start = timeit.default_timer()\n",
    "#             print('started '+name+str(size)+' #'+str(i))\n",
    "        \n",
    "#             original = '_original'+str(i)\n",
    "#             pdfs_use = make_instantiation(name, size, pdfs, bonus=original)\n",
    "# #             plot = plot_examples(size, name, bonus=original)\n",
    "# #             top_bonusdict[original] = ['-', 0.25]\n",
    "        \n",
    "#             z_grid = dataset_info[name]['in_z_grid']\n",
    "#             N_comps = dataset_info[name]['N_GMM']\n",
    "        \n",
    "#             postfit = '_postfit'+str(i)\n",
    "#             catalog = setup_from_grid(name, pdfs_use, z_grid, N_comps, high_res=high_res, bonus=postfit)\n",
    "# #             plot = plot_examples(size, name, bonus=postfit)\n",
    "# #             top_bonusdict[postfit] = ['-', 0.5]\n",
    "        \n",
    "#             for n_floats_use in floats:\n",
    "# #                 bonusdict = top_bonusdict.copy()\n",
    "#                 float_start = timeit.default_timer()\n",
    "#                 print('started '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use))\n",
    "        \n",
    "#                 ensembles = analyze_individual(catalog, z_grid, n_floats_use, name, n_moments_use, i=i, bonus=postfit)\n",
    "                \n",
    "# #                 for f in formats:\n",
    "# #                     fname = str(n_floats_use)+f+str(i)\n",
    "# #                     plot = plot_examples(size, name, bonus=fname)\n",
    "# #                     bonusdict[fname] = [styles[f], 0.5]\n",
    "# #                 plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "# #                 plot = plot_individual_kld(size, name, n_floats_use, i=i)\n",
    "            \n",
    "#                 stack_evals = analyze_stacked(catalog, ensembles, z_grid, n_floats_use, name, i=i)\n",
    "# #                 plot = plot_estimators(size, name, n_floats_use, i=i)\n",
    "            \n",
    "#                 print('FINISHED '+name+str(size)+' #'+str(i)+' with '+str(n_floats_use)+' in '+str(timeit.default_timer() - float_start))\n",
    "#             print('FINISHED '+name+str(size)+' #'+str(i)+' in '+str(timeit.default_timer() - i_start))\n",
    "# #         plot = plot_pz_metrics(name, size)\n",
    "# #         plot = plot_pz_delta_moments(name, size)      \n",
    "# #         plot = plot_nz_klds(name, size)\n",
    "# #         plot = plot_nz_moments(name, size)\n",
    "        \n",
    "#         print('FINISHED '+name+str(size)+' in '+str(timeit.default_timer() - size_start))\n",
    "        \n",
    "#     print('FINISHED '+name+' in '+str(timeit.default_timer() - dataset_start))\n",
    "# print('FINISHED everything in '+str(timeit.default_timer() - global_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plots to share axes, enabling combination of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = [3, 10, 30, 100]\n",
    "sizes = [100]#[10, 100, 1000]\n",
    "names = dataset_info.keys()\n",
    "instantiations = range(0, 10)\n",
    "\n",
    "all_randos = [[np.random.choice(size, n_plot, replace=False) \n",
    "               for size in sizes] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this a more clever structure, i.e. a dict\n",
    "colors = {'quantiles': 'darkviolet', 'histogram': 'darkorange', 'samples': 'g'}\n",
    "styles = {'quantiles': '--', 'histogram': ':', 'samples': '-.'}\n",
    "stepstyles = {'quantiles': 'dashed', 'histogram': 'dotted', 'samples': 'dashdot'}\n",
    "\n",
    "colors_plus = colors.copy()\n",
    "colors_plus['truth'] = 'black'\n",
    "styles_plus = styles.copy()\n",
    "styles_plus['truth'] = '-'\n",
    "\n",
    "iqr_min = [3.5]\n",
    "iqr_max = [delta]\n",
    "modes_max = [0]\n",
    "pz_max = [1.]\n",
    "nz_max = [1.]\n",
    "hist_max = [1.]\n",
    "dist_min = [0.]\n",
    "dist_max = [0.]\n",
    "pz_mean_max = -10.*np.ones(n_moments_use)\n",
    "pz_mean_min = 10.*np.ones(n_moments_use)\n",
    "kld_min = [1.]\n",
    "kld_max = [1.]\n",
    "indie_delta_kld_min = [1.]\n",
    "indie_delta_kld_max = [-1.]\n",
    "nz_mean_max = -10.*np.ones(n_moments_use)\n",
    "nz_mean_min = 10.*np.ones(n_moments_use)\n",
    "n_delta_max = -10.*np.ones(n_moments_use)\n",
    "n_delta_min = 10.*np.ones(n_moments_use)\n",
    "\n",
    "norm = False#true for shared axes on individual instantiation plots, otherwise false\n",
    "\n",
    "moments_to_save = ['pz_kld_moments', 'pz_moments', 'pz_moment_deltas', 'nz_moments']\n",
    "metrics_to_save = ['nz_klds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comment out for NERSC\n",
    "# set norm to True and run twice to match axis limits\n",
    "\n",
    "for name in names:\n",
    "    for size in sizes:\n",
    "#         for stat_name in moments_to_save + metrics_to_save:\n",
    "#             clear_stats(name, size, stat_name)\n",
    "#         for i in instantiations:\n",
    "#             top_bonusdict = {}\n",
    "#             bo = '_original'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bo, norm=norm)\n",
    "#             top_bonusdict[bo] = ['-', 0.25]\n",
    "#             bp = '_postfit'+str(i)\n",
    "#             plot = plot_examples(size, name, bonus=bp, norm=norm)\n",
    "#             top_bonusdict[bp] = ['-', 0.5]\n",
    "#             for n in range(len(floats)):\n",
    "#                 bonusdict = top_bonusdict.copy()\n",
    "#                 n_floats_use = floats[n]\n",
    "#                 for f in formats:\n",
    "#                     fname = str(n_floats_use)+f+str(i)\n",
    "#                     plot = plot_examples(size, name, bonus=fname, norm=norm)\n",
    "#                     bonusdict[fname] = [styles[f], 0.5]\n",
    "#                 plot = plot_all_examples(name, size, n_floats_use, i, bonus=bonusdict)\n",
    "#                 plot = plot_individual_kld(size, name, n_floats_use, i)\n",
    "#                 plot = plot_estimators(size, name, n_floats_use, i)\n",
    "#                 for stat_name in moments_to_save:\n",
    "#                     save_moments_wrapper(name, size, n_floats_use, i, stat_name)\n",
    "#                 for stat_name in metrics_to_save:\n",
    "#                     save_metrics_wrapper(name, size, n_floats_use, i, stat_name)\n",
    "#         plot = plot_kld_stats(name, size)\n",
    "#         plot = plot_pz_metrics(name, size)\n",
    "#         plot = plot_pz_delta_moments(name, size)\n",
    "        plot = plot_nz_klds(name, size)\n",
    "#         plot = plot_nz_moments(name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def just_modality(dataset_key, n_gals_use, bonus=None):\n",
    "#     import scipy.signal\n",
    "#     path = os.path.join(dataset_key, str(n_gals_use))\n",
    "#     loc = os.path.join(path, 'pzs'+str(n_gals_use)+dataset_key+bonus)\n",
    "#     with open(loc+'.hkl', 'r') as filename:\n",
    "#         info = hickle.load(filename)\n",
    "#         pdfs_use = info['pdfs']\n",
    "#     modality, iqrs = [], []\n",
    "#     dpdfs = pdfs_use[:,1:] - pdfs_use[:,:-1]\n",
    "#     ddpdfs = dpdfs[:, 1:] - dpdfs[:, :-1]\n",
    "#     for i in range(n_gals_use):\n",
    "#         modality.append(len(scipy.signal.argrelmax(pdfs_use[i])[0]))#(len(np.where(np.signbit(ddpdfs[i]))[0]))\n",
    "#         cdf = np.cumsum(qp.utils.normalize_integral((dataset_info[dataset_key]['z_grid'], pdfs_use[i]), vb=False)[1])\n",
    "#         iqr_lo = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.25)]\n",
    "#         iqr_hi = dataset_info[dataset_key]['z_grid'][bisect.bisect_left(cdf, 0.75)]\n",
    "#         iqrs.append(iqr_hi - iqr_lo)\n",
    "# #     modality = np.array(modality)\n",
    "# #     iqrs = np.array(iqrs)\n",
    "\n",
    "# #     loc = os.path.join(path, 'modality'+str(n_gals_use)+dataset_key+bonus)\n",
    "# #     with open(loc+'.hkl', 'w') as filename:\n",
    "# #         info = {}\n",
    "# #         info['modes'] = modality\n",
    "# #         info['iqrs'] = iqrs\n",
    "# #         hickle.dump(info, filename)\n",
    "#     return(modality, iqrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_modes, all_iqrs = {}, {}\n",
    "# for name in names:\n",
    "#     all_modes[name], all_iqrs[name] = {}, {}\n",
    "#     for size in sizes:\n",
    "#         all_modes[name][str(size)], all_iqrs[name][str(size)] = [], []\n",
    "#         for i in instantiations:\n",
    "# #         print_nz_moments(name, size)\n",
    "#             original = '_original'+str(i)\n",
    "#             (modality, iqrs) = just_modality(name, size, bonus=original)\n",
    "#             all_modes[name][str(size)].append(modality)\n",
    "#             all_iqrs[name][str(size)].append(iqrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in names:\n",
    "#     for size in sizes:\n",
    "#         modality = np.array(all_modes[name][str(size)]).flatten()\n",
    "#         modality_cdf = []\n",
    "#         modegrid = range(np.max(modality))\n",
    "#         for x in modegrid:\n",
    "#             modality_cdf.append(len(modality[modality==x]))\n",
    "#         plt.hist(modality, normed=True)\n",
    "#         plt.title(name+str(size)+'modality'+str(np.median(modality)))\n",
    "#         plt.show()\n",
    "#         plt.close()\n",
    "#         print(zip(modegrid, modality_cdf))\n",
    "# #         iqrdist = np.array(all_iqrs[name][str(size)]).flatten()\n",
    "# #         plt.title(name+str(size)+'iqrdist'+str(np.median(iqrdist)))\n",
    "# #         plt.hist(iqrdist, normed=True)\n",
    "# #         plt.show()\n",
    "# #         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thing = load_one_stat('ss', 100, 3, 0, 'pz_moment_deltas')\n",
    "# print(np.mean(np.shinape(thing['quantiles']), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_moments('ss', 100, 3, thing, 'pz_moment_deltas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join('ss', str(100))\n",
    "# loc = os.path.join(path, 'pz_moment_deltas'+str(100)+'ss')\n",
    "# with open(loc+'.hkl', 'r') as pz_file:\n",
    "#     pz_stats = hickle.load(pz_file)\n",
    "    \n",
    "# print(np.shape(pz_stats['quantiles'][0]))#N_f * n_m * n_i * n_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified = np.array(pz_stats['quantiles']).reshape(4, 4, 1000)*100.\n",
    "# print(np.shape(modified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(np.array(pz_stats[f][0]).reshape(4, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_modified = modified * 100.\n",
    "# mean = np.mean(more_modified, axis=-1)\n",
    "# print(mean)\n",
    "# std = np.std(more_modified, axis=-1)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(np.shape(modified))\n",
    "# # plt.hist(modified[0][3])\n",
    "# weird_x = np.log(np.array(floats))\n",
    "\n",
    "# moment_num = 3\n",
    "# for s in range(3):\n",
    "#     f = formats[s]\n",
    "#     const = 0.1\n",
    "#     f_factor = const * (s - 1)\n",
    "#     new_data = np.array(pz_stats[f][moment_num]).reshape(4, 1000)*100.\n",
    "#     plt.plot(np.exp(weird_x+f_factor), np.median(new_data, axis=-1), linestyle=styles[f], marker=moment_shapes[moment_num], mfc='none', markersize=5, alpha=0.5, color=colors[f])\n",
    "#     violin = plt.violinplot(list(new_data), np.exp(weird_x+f_factor), showextrema=False, showmeans=False, showmedians=False, widths=np.exp(weird_x+const/2.)-np.exp(weird_x))\n",
    "# #     for partname in ['cmedians']:\n",
    "# #         vp = violin[partname]\n",
    "# #         vp.set_edgecolor(colors[f])\n",
    "# #         vp.set_linewidth(3)\n",
    "# # Make the violin body blue with a red border:\n",
    "#     for vp in violin['bodies']:\n",
    "#         vp.set_facecolor(coplors[f])\n",
    "# #         vp.set_edgecolor('k')\n",
    "# #         vp.set_linewidth(0)\n",
    "#         vp.set_alpha(0.5)\n",
    "# plt.semilogx()\n",
    "# plt.ylim(-50., 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.boxplot(list(new_data), floats, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(pz_stats['quantiles'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(violin.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(plt.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (not)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
