{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring BPZ Test Data\n",
    "\n",
    "_Alex Malz (NYU) & Phil Marshall (SLAC)_\n",
    "\n",
    "In this notebook we develop machinery to evaluate our approximations on whole datasets in \"survey mode.\" \n",
    "\n",
    "_This notebook is intended to work with [qp v0.2-beta](https://zenodo.org/badge/latestdoi/73841220) and may not work with other versions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "    \n",
    "import hickle\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import random\n",
    "import cProfile\n",
    "import pstats\n",
    "import StringIO\n",
    "import timeit\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import qp\n",
    "from qp.utils import calculate_kl_divergence as make_kld\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "random.seed(a=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up, Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two datasets available:\n",
    "\n",
    "* $10^{5}$ LSST-like mock data provided by Sam Schmidt (UC Davis, LSST\n",
    "* $10^{4}$ Euclid-like mock data provided by Melissa Graham (UW, LSST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose one of these:\n",
    "# dataset_key = 'Euclid'# Melissa Graham's data\n",
    "dataset_key = 'LSST'# Sam Schmidt's data\n",
    "dataname = dataset_key\n",
    "\n",
    "dataset_info = {}\n",
    "\n",
    "dataset_info[dataset_key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets are fit with BPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset_key == 'Euclid':\n",
    "    datafilename = 'bpz_euclid_test_10_2.probs'\n",
    "elif dataset_key == 'LSST':\n",
    "    datafilename = 'test_magscat_trainingfile_probs.out'\n",
    "    \n",
    "dataset_info[dataset_key]['filename'] = datafilename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files don't appear to come with information about the native format or metaparameters, but we are told they're evaluations on a regular grid of redshifts with given endpoints and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_key == 'Euclid':\n",
    "    z_low = 0.01\n",
    "    z_high = 3.51\n",
    "elif dataset_key == 'LSST':\n",
    "    z_low = 0.005\n",
    "    z_high = 2.11\n",
    "    \n",
    "dataset_info[dataset_key]['z_lim'] = (z_low, z_high)\n",
    "\n",
    "z_grid = np.arange(z_low, z_high, 0.01, dtype='float')\n",
    "z_range = z_high - z_low\n",
    "delta_z = z_range / len(z_grid)\n",
    "\n",
    "dataset_info[dataset_key]['z_grid'] = z_grid\n",
    "dataset_info[dataset_key]['delta_z'] = delta_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the catalog data.  Note that it has a sizeable footprint even for a \"small\" number of galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warning: reading in the data is slow for Sam Schmidt's dataset!\n",
    "with open(dataset_info[dataset_key]['filename'], 'rb') as data_file:\n",
    "    lines = (line.split(None) for line in data_file)\n",
    "    lines.next()\n",
    "    pdfs = np.array([[float(line[k]) for k in range(1,len(line))] for line in lines])\n",
    "\n",
    "# dataset_info[dataset_key]['native_pdfs'] = pdfs\n",
    "\n",
    "print('storage footprint '+str(sys.getsizeof(pdfs))+' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the BPZ $p(z)$'s\n",
    "\n",
    "Let's plot a few interesting PDFs from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['red','green','blue','cyan','magenta','yellow']\n",
    "# n_plot = len(colors)\n",
    "\n",
    "# # if dataset_key == 'mg':\n",
    "# #     indices = [1, 3, 14, 16, 19, 21]\n",
    "# # elif dataset_key == 'ss':\n",
    "# n_gals_tot = len(pdfs)\n",
    "# full_gal_range = range(n_gals_tot)\n",
    "# indices = np.random.choice(full_gal_range, n_plot)\n",
    "\n",
    "# for i in range(n_plot):\n",
    "#     plt.plot(dataset_info[dataset_key]['z_grid'], pdfs[indices[i]], \n",
    "#              color=colors[i], label=dataset_key+' #'+str(indices[i]))\n",
    "# plt.xlabel(r'$z$', fontsize=16)\n",
    "# plt.ylabel(r'$p(z)$', fontsize=16)\n",
    "# plt.title(dataset_key+' mock catalog')\n",
    "# plt.legend()\n",
    "# plt.savefig('pz_placeholder_'+dataset_key+'.pdf', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: BPZ PDFs are not properly normalized. In order to be true PDFs, we want $\\int_{-\\infty}^{\\infty} p(z) dz = 1$, but the data file entries satisfy $\\sum _{z=z_min}^{z_{max}} p(z) = 1$, which is not in general the same.  `qp` approximates the desired integral as $1 = \\int p(z) dz \\approx \\Delta_{z} \\sum_{z=z_{min}}^{z_{max}} p(z)$ where $\\Delta_{z} = \\frac{z_{max} - z_{min}}{N_{ff}}$, where the native format PDF is evaluated at $N_{ff}$ redshifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Approximating the BPZ $p(z)'s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick out a galaxy with an interesting $p(z)$ to turn into a `qp.PDF` object initialized with a gridded parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_key == 'Euclid':\n",
    "    chosen = 5390\n",
    "elif dataset_key == 'LSST':\n",
    "#     chosen = 108019 \n",
    "    indices = [ 12543,  52661,  46216,  53296,  95524,  84574 ,  2607  ,56017 , 64794, 7600]\n",
    "    chosen = indices[9]\n",
    "    \n",
    "start_time = timeit.default_timer()\n",
    "G = qp.PDF(gridded=(dataset_info[dataset_key]['z_grid'], pdfs[chosen]))\n",
    "print(timeit.default_timer() - start_time)\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qp` cannot currently convert gridded PDFs to histograms or quantiles - we need to make a GMM first, and use this to instantiate a `qp.PDF` object using a `qp.composite` object based on that GMM as `qp.PDF.truth`.  The number of parameters necessary for a qualitatively good fit depends on the characteristics of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_key == 'Euclid':\n",
    "    nc_needed = 3\n",
    "elif dataset_key == 'LSST':\n",
    "    nc_needed = 5\n",
    "    \n",
    "dataset_info[dataset_key]['N_GMM'] = nc_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a GMM directly to the gridded PDF (via an internal interpolation).  The direct fit, however, is not guaranteed to converge, particularly if the underlying distribution is not actually well-described by a weighted sum of Gaussians -- this is why storing the GMM parameters instead of a non-parametric format can be dangerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "G.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "              using='gridded', vb=True)\n",
    "time = timeit.default_timer() - start_time\n",
    "print(str(time)+' for GMM fit to gridded')\n",
    "\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative is to take a large number of samples and fit a GMM to those (via the same internal interpolation).  We can check that the fits are very similar.  Though it is slower, we will sample before fitting to guarantee convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_res = 1000\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "G.sample(high_res, vb=False)\n",
    "G.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                       using='samples', vb=True)\n",
    "time = timeit.default_timer() - start_time\n",
    "print(str(time)+' for GMM fit to samples')\n",
    "\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `qp.composite` object can be used as the `qp.PDF.truth` to initialize a new `qp.PDF` object that doesn't have any information about the gridded or sample approximations but has a qualitatively similar shape and is thus \"realistically complex\" enough to draw conclusions about real data.  Now we can approximate it any way we like!  Consider this example for $N_f=7$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_f = 7\n",
    "M = qp.PDF(truth=G.mix_mod, limits=dataset_info[dataset_key]['z_lim'])\n",
    "M.quantize(N=N_f, vb=False)\n",
    "M.histogramize(N=N_f, binrange=dataset_info[dataset_key]['z_lim'], vb=False)\n",
    "M.sample(N=N_f, using='truth', vb=False)\n",
    "M.plot(loc=dataset_key+'_example_pz.pdf', vb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Quantifying the Accuracy of the Approximation\n",
    "\n",
    "We can also calculate the KLD metric on this `qp.PDF`.  The KLD quantifies the information loss of an approximation of a PDF relative to the true PDF in units of nats.  Thus, a lower KLD corresponds to more information being preserved in the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = ['quantiles', 'histogram', 'samples']\n",
    "parametrizations = {}\n",
    "for f in formats:\n",
    "    parametrizations[f] = {}\n",
    "    for ff in formats:\n",
    "        parametrizations[f][ff] = None\n",
    "parametrizations['quantiles']['quantiles'] = M.quantiles\n",
    "parametrizations['histogram']['histogram'] = M.histogram\n",
    "parametrizations['samples']['samples'] = M.samples\n",
    "\n",
    "dataset_info[dataset_key]['inits'] = parametrizations\n",
    "\n",
    "klds = {}\n",
    "P = qp.PDF(truth=M.truth)\n",
    "for f in formats:\n",
    "    Q = qp.PDF(quantiles=dataset_info[dataset_key]['inits'][f]['quantiles'], \n",
    "                histogram=dataset_info[dataset_key]['inits'][f]['histogram'], \n",
    "                samples=dataset_info[dataset_key]['inits'][f]['samples'])\n",
    "    klds[f] = make_kld(P, Q)\n",
    "\n",
    "print(klds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Mode\n",
    "\n",
    "We want to compare parametrizations for large catalogs, so we'll need to be more efficient.  The `qp.Ensemble` object is a wrapper for `qp.PDF` objects enabling conversions to be performed and metrics to be calculated in parallel.  We'll experiment on a subsample of 100 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gals_tot = len(pdfs)\n",
    "n_gals_use = 100\n",
    "full_gal_range = range(n_gals_tot)\n",
    "subset = np.random.choice(full_gal_range, n_gals_use)\n",
    "pdfs_use = pdfs[subset]\n",
    "\n",
    "# using the same grid for output as the native format, but doesn't need to be so\n",
    "dataset_info[dataset_key]['in_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "dataset_info[dataset_key]['metric_z_grid'] = dataset_info[dataset_key]['z_grid']\n",
    "n_floats_use = 10\n",
    "\n",
    "if dataset_key == 'Euclid':\n",
    "    dataset_info[dataset_key]['N_GMM'] = 3\n",
    "elif dataset_key == 'LSST':\n",
    "    dataset_info[dataset_key]['N_GMM'] = 5\n",
    "fit_components = dataset_info[dataset_key]['N_GMM']\n",
    "\n",
    "n_moments_use = 3\n",
    "\n",
    "colors = {'quantiles':'b', 'histogram':'r', 'samples':'g'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading in our catalog of gridded PDFs, sampling them, fitting GMMs to the samples, and establishing a new `qp.Ensemble` object where each meber `qp.PDF` object has `qp.PDF.truth`$\\neq$`None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_from_grid(in_pdfs, z_grid, N_comps, high_res=1000):\n",
    "    \n",
    "    #read in the data, happens to be gridded\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    N_pdfs = len(in_pdfs)\n",
    "    \n",
    "#     plot_examples(N_pdfs, z_grid, pdfs)\n",
    "    \n",
    "    print('making the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    E0 = qp.Ensemble(N_pdfs, gridded=(z_grid, in_pdfs), vb=True)\n",
    "    print('made the initial ensemble of '+str(N_pdfs)+' PDFs')\n",
    "    \n",
    "    #fit GMMs to gridded pdfs based on samples (faster than fitting to gridded)\n",
    "    print('sampling for the GMM fit')\n",
    "    samparr = E0.sample(high_res, vb=False)\n",
    "    print('took '+str(high_res)+' samples')\n",
    "    \n",
    "    print('making a new ensemble from samples')\n",
    "    Ei = qp.Ensemble(N_pdfs, samples=samparr, vb=False)\n",
    "    print('made a new ensemble from samples')\n",
    "    \n",
    "    print('fitting the GMM to samples')\n",
    "    GMMs = Ei.mix_mod_fit(comps=N_comps, vb=False)\n",
    "    print('fit the GMM to samples')\n",
    "    \n",
    "    #set the GMMS as the truth\n",
    "    print('making the final ensemble')\n",
    "    Ef = qp.Ensemble(N_pdfs, truth=GMMs, vb=False)\n",
    "    print('made the final ensemble')\n",
    "    \n",
    "    return(Ef)\n",
    "#     return\n",
    "\n",
    "def plot_examples(z_grid, pdfs, n_plot=6):\n",
    "    N_pdfs =len(pdfs)\n",
    "    randos = np.random.choice(range(N_pdfs), n_plot)\n",
    "    for i in range(n_plot):\n",
    "        plt.plot(z_grid, pdfs[randos[i]], label=dataset_key+r'\\#'+str(randos[i]))\n",
    "    plt.xlabel(r'$z$', fontsize=16)\n",
    "    plt.ylabel(r'$p(z)$', fontsize=16)\n",
    "    plt.title(dataset_key+' mock catalog')\n",
    "    plt.legend()\n",
    "    plt.savefig('pz_placeholder_'+dataset_key+'.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "catalog = setup_from_grid(pdfs_use, dataset_info[dataset_key]['in_z_grid'], \n",
    "                          fit_components)\n",
    "\n",
    "# pr.disable()\n",
    "# s = StringIO.StringIO()\n",
    "# sortby = 'cumtime'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())\n",
    "\n",
    "plot_examples(dataset_info[dataset_key]['in_z_grid'], pdfs_use, n_plot=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the KLD between each approximation and the truth for every member of the ensemble.  We make the `qp.Ensemble.kld` into a `qp.PDF` object of its own to compare the moments of the KLD distributions for different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_individual(E, z_grid, N_floats, N_moments=4):\n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    Eq, Eh, Es = E, E, E\n",
    "    inits = {}\n",
    "    for f in formats:\n",
    "        inits[f] = {}\n",
    "        for ff in formats:\n",
    "            inits[f][ff] = None\n",
    "            \n",
    "    print('performing quantization')\n",
    "    inits['quantiles']['quantiles'] = Eq.quantize(N=N_floats, vb=False)\n",
    "    print('performing histogramization')\n",
    "    inits['histogram']['histogram'] = Eh.histogramize(N=N_floats, binrange=zlim, vb=False)\n",
    "    print('performing sampling')\n",
    "    inits['samples']['samples'] = Es.sample(samps=N_floats, vb=False)\n",
    "        \n",
    "    print('making the approximate ensembles')\n",
    "    Eo ={}\n",
    "    for f in formats:\n",
    "        Eo[f] = qp.Ensemble(E.n_pdfs, truth=E.truth, \n",
    "                            quantiles=inits[f]['quantiles'], \n",
    "                            histogram=inits[f]['histogram'],\n",
    "                            samples=inits[f]['samples'])\n",
    "    print('made the approximate ensembles')\n",
    "    \n",
    "    print('calculating the individual metrics')\n",
    "    klds = {}\n",
    "    metrics = {}\n",
    "    moments = {}\n",
    "    \n",
    "    for key in Eo.keys():\n",
    "        print('starting '+key)\n",
    "        klds[key] = Eo[key].kld(using=key, limits=zlim, dx=delta_z)\n",
    "        samp_metric = qp.PDF(samples=klds[key])\n",
    "        gmm_metric = samp_metric.mix_mod_fit(n_components=dataset_info[dataset_key]['N_GMM'], \n",
    "                                             using='samples')\n",
    "        metrics[key] = qp.PDF(truth=gmm_metric)\n",
    "        moments[key] = []\n",
    "        for n in range(N_moments+1):\n",
    "            moments[key].append([qp.utils.calculate_moment(metrics[key], n,\n",
    "                                                          using=key, \n",
    "                                                          limits=zlim, \n",
    "                                                          dx=delta_z, \n",
    "                                                          vb=False)])\n",
    "        print('finished with '+key)\n",
    "    print('calculated the individual metrics')\n",
    "    \n",
    "#     plot_individual(klds, N_floats)\n",
    "    \n",
    "    return(Eo, klds, moments)\n",
    "\n",
    "def plot_individual(pz_klds, N_floats):\n",
    "    colors = {'quantiles':'b', 'histogram':'r', 'samples':'g'}\n",
    "    plot_bins = np.linspace(-3., 3., 20)\n",
    "    for key in pz_klds.keys():\n",
    "        plt.hist(np.log(pz_klds[key]), color=colors[key], alpha=0.5, \n",
    "             label=key, normed=True, bins=plot_bins)\n",
    "    plt.legend()\n",
    "    plt.ylabel('frequency')\n",
    "    plt.xlabel(r'$\\log[KLD]$')\n",
    "    plt.title(dataset_key+r' dataset with $N_{f}='+str(N_floats)+r'$')\n",
    "    plt.savefig(dataset_key+'_metric_histogram_placeholder.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "(ensembles, pz_klds, metric_moments) = analyze_individual(catalog, \n",
    "                                                          dataset_info[dataset_key]['metric_z_grid'], \n",
    "                                                          n_floats_use, \n",
    "                                                          n_moments_use)\n",
    "dataset_info[dataset_key]['pz_klds'] = pz_klds\n",
    "dataset_info[dataset_key]['pz_kld_moments'] = metric_moments\n",
    "plot_individual(pz_klds, n_floats_use)\n",
    "\n",
    "# pr.disable()\n",
    "# s = StringIO.StringIO()\n",
    "# sortby = 'cumtime'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate metrics on the stacked estimator $\\hat{n}(z)$ that is the average of all members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_stacked(E0, E, z_grid):\n",
    "    \n",
    "    zlim = (min(z_grid), max(z_grid))\n",
    "    z_range = zlim[-1] - zlim[0]\n",
    "    delta_z = z_range / len(z_grid)\n",
    "    \n",
    "    parametrizations = E.keys()\n",
    "    print('stacking the ensembles')           \n",
    "    stacked_pdfs = {}\n",
    "    for key in formats:\n",
    "        stacked_pdfs[key] = qp.PDF(gridded=E[key].stack(z_grid, using=key, \n",
    "                                                        vb=False)[key])\n",
    "    \n",
    "    stacked_pdfs['truth'] = qp.PDF(gridded=E0.stack(z_grid, using='truth', \n",
    "                                                    vb=False)['truth'])\n",
    "    print('stacked the ensembles')\n",
    "    \n",
    "    print('calculating the metrics')\n",
    "    klds = {}\n",
    "    for key in parametrizations:\n",
    "        klds[key] = qp.utils.calculate_kl_divergence(stacked_pdfs['truth'],\n",
    "                                                     stacked_pdfs[key], \n",
    "                                                     limits=zlim, dx=delta_z)\n",
    "    print('calculated the metrics')\n",
    "    \n",
    "#     plot_estimators(z_grid, stacked_pdfs, klds)\n",
    "    \n",
    "    return(stacked_pdfs, klds)\n",
    "\n",
    "def plot_estimators(z_grid, stacked_pdfs, klds):\n",
    "    colors = {'quantiles':'b', 'histogram':'r', 'samples':'g'}\n",
    "    plt.title(r'$\\hat{n}(z)$ for '+str(n_floats_use)+' numbers')\n",
    "    plt.plot(z_grid, stacked_pdfs['truth'].evaluate(z_grid, vb=False)[1], color='black', lw=4, alpha=0.3, label='truth')\n",
    "    for key in formats:\n",
    "        plt.plot(z_grid, stacked_pdfs[key].evaluate(z_grid, vb=False)[1], label=key+' KLD='+str(klds[key]), color=colors[key])\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\hat{n}(z)$')\n",
    "    plt.legend()\n",
    "    plt.title(r'$\\hat{n}(z)$ for '+str(n_floats_use)+' numbers')\n",
    "    plt.savefig(dataset_key+'_nz_comparison.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "(stack_evals, nz_klds) = analyze_stacked(catalog, ensembles, dataset_info[dataset_key]['metric_z_grid'])\n",
    "dataset_info[dataset_key]['nz_ests'] = stack_evals\n",
    "dataset_info[dataset_key]['nz_klds'] = nz_klds\n",
    "plot_estimators(dataset_info[dataset_key]['metric_z_grid'], stack_evals, nz_klds)\n",
    "\n",
    "# pr.disable()\n",
    "# s = StringIO.StringIO()\n",
    "# sortby = 'cumtime'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so we can remake the plots later without running everything again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We'd like to do this for many values of $N_{f}$ as well as larger catalog subsamples, repeating the analysis many times to establish error bars on the KLD as a function of format, $N_{f}$, and dataset.  The things we want to plot across multiple datasets/number of parametes are:\n",
    "\n",
    "1. KLD of stacked estimator, i.e. `N_f` vs. `nz_output[dataset][format][instantiation][KLD_val_for_N_f]`\n",
    "2. moments of KLD of individual PDFs, i.e. `n_moment, N_f` vs. `pz_output[dataset][format][n_moment][instantiation][moment_val_for_N_f]`\n",
    "\n",
    "So, we ned to make sure these are saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('nz_metrics.hkl'):\n",
    "    with open('nz_metrics.hkl', 'r') as nz_file:\n",
    "        #read in content of list/dict\n",
    "        nz_stats = hickle.load(nz_file)\n",
    "else:\n",
    "    nz_stats = {}\n",
    "    nz_stats['N_f'] = []\n",
    "    \n",
    "if N_f not in nz_stats['N_f']:\n",
    "    nz_stats['N_f'].append(N_f)\n",
    "where_N_f = nz_stats['N_f'].index(N_f)\n",
    "    \n",
    "if dataset_key not in nz_stats.keys():\n",
    "    nz_stats[dataset_key] = {}\n",
    "    for f in parametrizations:#change this name to formats\n",
    "        nz_stats[dataset_key][f] = [[]]\n",
    "        \n",
    "for f in parametrizations:\n",
    "    nz_stats[dataset_key][f][where_N_f].append(dataset_info[dataset_key]['nz_klds'][f])\n",
    "\n",
    "with open('nz_metrics.hkl', 'w') as nz_file:\n",
    "    hickle.dump(nz_stats, nz_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the KLD on $\\hat{n}(z)$ for all formats as $N_{f}$ changes.  We want to repeat this for many subsamples of the catalog to establush error bars on the KLD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nz_metrics.hkl', 'r') as nz_file:\n",
    "    nz_stats = hickle.load(nz_file)\n",
    "\n",
    "colors = {'quantiles':'b', 'histogram':'r', 'samples':'g'}\n",
    "\n",
    "# need to get some version of this working from nz_klds\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for f in parametrizations.keys():\n",
    "    data_arr = np.swapaxes(np.array(nz_stats[dataset_key][f]), 0, 1)#turn N_f * instantiations into instantiations * N_f\n",
    "    n_i = len(data_arr)\n",
    "    a = 1./n_i\n",
    "    plt.plot([2 * max(nz_stats['N_f']), 2 * max(nz_stats['N_f'])], [1., 10.], color=colors[f], alpha=a, label=f)\n",
    "    for i in data_arr:\n",
    "        # will be regular plot not scatter with more N_f options\n",
    "        plt.plot(nz_stats['N_f'], i[0], color=colors[f], alpha=a)\n",
    "\n",
    "plt.semilogy()\n",
    "plt.semilogx()\n",
    "plt.xlim(min(nz_stats['N_f'])-1, max(nz_stats['N_f'])+1)\n",
    "plt.ylim(1., 10.)\n",
    "plt.xlabel(r'number of parameters')\n",
    "plt.ylabel(r'KLD')\n",
    "plt.legend()\n",
    "plt.title(r'$\\hat{n}(z)$ KLD on '+str(n_gals_use)+' from '+dataset_key)\n",
    "plt.savefig(dataset_key+'_nz_metrics_placeholder.png', dpi=250)\n",
    "\n",
    "# won't really know how this looks without more N_f tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot the moments of the KLD distribution for each format as $N_{f}$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('pz_metrics.hkl'):\n",
    "    with open('pz_metrics.hkl', 'r') as pz_file:\n",
    "        #read in content of list/dict\n",
    "        pz_stats = hickle.load(pz_file)\n",
    "else:\n",
    "    pz_stats = {}\n",
    "    pz_stats['N_f'] = []\n",
    "    \n",
    "if N_f not in pz_stats['N_f']:\n",
    "    pz_stats['N_f'].append(N_f)\n",
    "where_N_f = pz_stats['N_f'].index(N_f)\n",
    "    \n",
    "if dataset_key not in pz_stats.keys():\n",
    "    pz_stats[dataset_key] = {}\n",
    "    for f in parametrizations:#change this name to formats\n",
    "        pz_stats[dataset_key][f] = []\n",
    "        for m in range(n_moments_use + 1):\n",
    "            pz_stats[dataset_key][f].append([[]])\n",
    "\n",
    "if N_f not in pz_stats['N_f']:\n",
    "    pz_stats[dataset_key][f][m].append([])\n",
    "        \n",
    "for f in parametrizations:\n",
    "    for m in range(n_moments_use + 1):\n",
    "        pz_stats[dataset_key][f][m][where_N_f].append(dataset_info[dataset_key]['pz_kld_moments'][f][m])\n",
    "\n",
    "with open('pz_metrics.hkl', 'w') as pz_file:\n",
    "    hickle.dump(pz_stats, pz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pz_metrics.hkl', 'r') as pz_file:\n",
    "    pz_stats = hickle.load(pz_file)\n",
    "\n",
    "def make_patch_spines_invisible(ax):\n",
    "    ax.set_frame_on(True)\n",
    "    ax.patch.set_visible(False)\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "\n",
    "shapes = ['o','+','x','v','^','<','>']\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(right=1.)\n",
    "\n",
    "ax_n = ax\n",
    "for key in parametrizations.keys():\n",
    "    ax_n.plot([-1], [0], color=colors[key], label=key)\n",
    "\n",
    "for n in range(1, 4):\n",
    "    ax.scatter([-1], [0], color='k', marker=shapes[n-1], label='moment '+str(n))\n",
    "    if n>1:\n",
    "        ax_n = ax.twinx()\n",
    "    if n>2:\n",
    "        ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (n-1)))\n",
    "        make_patch_spines_invisible(ax_n)\n",
    "        ax_n.spines[\"right\"].set_visible(True)\n",
    "    for f in parametrizations.keys():\n",
    "        data_arr = np.swapaxes(np.array(pz_stats[dataset_key][f][n]), 0, 1)\n",
    "        n_i = len(data_arr)\n",
    "        a = 1./n_i\n",
    "        for i in data_arr:\n",
    "            ax_n.scatter(pz_stats['N_f'], i, marker=shapes[n-1], color=colors[f], alpha=a)\n",
    "    ax_n.set_ylabel('moment '+str(n))\n",
    "ax.set_xlim(1,1000)#should be N_f range and logged\n",
    "ax.semilogx()\n",
    "ax.set_xlabel('number of parameters')\n",
    "ax.legend()\n",
    "fig.suptitle('KLD moments on '+str(n_gals_use)+' from '+dataset_key)\n",
    "fig.savefig(dataset_key+'_pz_metrics_placeholder.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Okay, now all I have to do is have this loop over both datasets, number of galaxies, and number of floats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything after here is scratch.  That's all, folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## everything works above here!  now it's time to make plots from this output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to test the experimental qp.Ensemble object!\n",
    "\n",
    "# def analyze():#(pdfs, N_comps, z, N_floats):\n",
    "    \n",
    "#     #read in the data, happens to be gridded\n",
    "#     z_low, z_high = min(z), max(z)\n",
    "#     N_pdfs = len(pdfs)\n",
    "#     out_E = {}\n",
    "#     E0 = qp.Ensemble(N_pdfs, gridded=(z, pdfs), vb=False)\n",
    "\n",
    "#     #fit gridded pdfs as GMMs based on samples\n",
    "#     samparr = E0.sample(1000, vb=False)\n",
    "#     print(np.shape(samparr))\n",
    "#     Ei = qp.Ensemble(N_pdfs, samples=samparr, vb=False)\n",
    "#     GMMs = Ei.mix_mod_fit(comps=N_comps, using='samples', vb=False)\n",
    "# #     out_E['GMMs'] = []\n",
    "# #     for GMM in GMMs:\n",
    "# #         out_E['GMMs'].append(GMM.functions[0].stats())\n",
    "    \n",
    "#     #set the GMMS as the truth\n",
    "#     Ef = qp.Ensemble(N_pdfs, truth=GMMs, vb=False)\n",
    "    \n",
    "#     #stack them and save the output\n",
    "#     out_E['truth'] = Ef.stack(z, using='mix_mod', vb=False)\n",
    "    \n",
    "# #     #evaluate as gridded and save the output\n",
    "# #     Et = qp.Ensemble(N_pdfs, gridded=Ef.evaluate(z))\n",
    "# #     out_E['gridded'] = Et.stack(z, using='gridded')\n",
    "    \n",
    "#     #evaluate as quantiles and save the output\n",
    "#     Eq = qp.Ensemble(N_pdfs, quantiles=Ef.quantize(N=N_floats), vb=False)\n",
    "#     #q_stack = Eq.stack(z, using='quantiles')\n",
    "#     out_E['quantiles'] = Eq.stack(z, using='quantiles', vb=False)\n",
    "    \n",
    "# #     #evaluate as histogram and save the output\n",
    "# #     Eh = qp.Ensemble(N_pdfs, histogram=Ef.histogramize(N=N_floats, binrange=(z_low, z_high)))\n",
    "# #     #h_stack = Eh.stack(z, using='histogram')\n",
    "# #     out_E['histogram'] = Eh.stack(z, using='histogram')\n",
    "    \n",
    "# #     #evaluate as samples and save the output\n",
    "# #     Es = qp.Ensemble(N_pdfs, samples=Ef.sample(samps=N_floats))\n",
    "# #     #s_stack = Es.stack(z, using='samples')\n",
    "# #     out_E['samples'] = Es.stack(z, using='samples')\n",
    "    \n",
    "#     return(out_E)#, KLDs, RMSEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a test with 100 galaxies and 10 parameters. This should take about 5 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n_gals_use, n_floats_use, s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the stacked versions and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.keys())\n",
    "# print(results['truth']['mix_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLDs, RMSEs = {}, {}\n",
    "\n",
    "# P = qp.PDF(gridded=results['truth']['mix_mod'])\n",
    "# metric_keys = results.keys()\n",
    "# metric_keys.remove('truth')\n",
    "\n",
    "# for est in metric_keys:\n",
    "#     Q = qp.PDF(gridded=results[est][est])\n",
    "#     KLDs[est] = qp.utils.calculate_kl_divergence(P, Q, vb=False)\n",
    "#     RMSEs[est] = qp.utils.calculate_rmse(P, Q, vb=False)\n",
    "#     plt.plot(results[est][est][0], results[est][est][1], label=est)\n",
    "# plt.legend()\n",
    "# print(KLDs, RMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Things are quite broken after this point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = qp.PDF(gridded=stack_ests['truth'])\n",
    "\n",
    "# KLDs, RMSEs = {}, {}\n",
    "# for est in .keys():\n",
    "#     Q = qp.PDF(gridded=stack_ests[est])\n",
    "#     KLDs[est] = qp.utils.calculate_kl_divergence(P, Q, vb=False)\n",
    "#     RMSEs[est] = qp.utils.calculate_rmse(P, Q, vb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the log standard deviations of the first component of the mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moments = np.array(results['stats']).T\n",
    "# fit_stats = moments[1]\n",
    "# plt.hist(np.log(fit_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of standard deviations of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = qp.PDF(samples = np.log(fit_stats))\n",
    "# T = D.mix_mod_fit(n_components=1)\n",
    "# D.plot()\n",
    "# print(np.exp(T.functions[0].stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now enough of the `qp.Ensemble` functionality has been implemented to merge into the `master` branch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ends the test of the experimental qp.Ensemble object\n",
    "# you may now return to your regularly scheduled programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_one(index, N_comps, z, N_floats, logfilename='logfile.txt', vb=False):\n",
    "#     \"\"\"\n",
    "#     Model the input BPZ P(z) as a GMM, approximate that GMM in \n",
    "#     various ways, and assess the quality of each approximation.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     index : int\n",
    "#         ID of galaxy\n",
    "#     N_comps : int\n",
    "#         Number of components used in GMM\n",
    "#     N_floats : int\n",
    "#         Number of floats used to parametrize the P(z)\n",
    "#     z : float, ndarr\n",
    "#         Redshift array for input gridded \"truth\". Used for \n",
    "#         evaluating n(z) too\n",
    "#     logfilename: string\n",
    "#         where to put logging information\n",
    "#     vb : boolean\n",
    "#         Verbose output?\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     result : dict\n",
    "#         Dictionary containing metric values, n(z) on standard \n",
    "#         grid, samples, \"true\" GMM gridded p(z).\n",
    "        \n",
    "#     Notes\n",
    "#     -----\n",
    "#     In some cases the GMM does not fit well, leading to bad KLD and \n",
    "#     RMSE values when it is compared to the truth.\n",
    "    \n",
    "#     \"\"\"\n",
    "# #     # Make z array if we don't already have it:\n",
    "# #     if z is None:\n",
    "# #         z = np.arange(0.01, 3.51, 0.01, dtype='float')\n",
    "#     dz = (max(z) - min(z)) / len(z)\n",
    "#     zlimits = [min(z), max(z)]\n",
    "\n",
    "#     # Make a dictionary to contain the results:     \n",
    "#     result = {}\n",
    "    \n",
    "#     # Make a GMM model of the input BPZ p(z) (which are stored\n",
    "#     # in the global 'pdfs' variable:\n",
    "#     G = qp.PDF(gridded=(z, pdfs[index]), vb=vb)\n",
    "    \n",
    "#     # Draw 1000 samples, fit a GMM model to them, and make a true PDF:\n",
    "#     G.sample(1000, vb=vb)\n",
    "#     GMM = G.mix_mod_fit(n_components=N_comps, vb=vb)\n",
    "#     P = qp.PDF(truth=GMM, vb=vb)\n",
    "    \n",
    "#     # Evaluate the GMM on the z grid, and store in the result dictionary. We'll \n",
    "#     # need this to make our \"true\" n(z) estimator. We don't need to keep the \n",
    "#     # z array, as we passed that in.\n",
    "#     result['truth'] = P.evaluate(z, using='truth', vb=vb)[1]\n",
    "\n",
    "#     # Now approximate P in various ways, and assess:\n",
    "#     Q, KLD, RMSE, approximation = {}, {}, {}, {}\n",
    "#     Q['quantiles'] = qp.PDF(quantiles=P.quantize(N=N_floats, vb=vb), vb=vb)\n",
    "#     Q['histogram'] = qp.PDF(histogram=P.histogramize(N=N_floats, binrange=zlimits, vb=vb), vb=vb)\n",
    "#     Q['samples'] = qp.PDF(samples=P.sample(N=N_floats, vb=vb), vb=vb)\n",
    "#     for k in Q.keys():\n",
    "#         KLD[k] = qp.calculate_kl_divergence(P, Q[k], limits=zlimits, dx=dz, vb=vb)\n",
    "#         RMSE[k] = qp.calculate_rmse(P, Q[k], limits=zlimits, dx=dz, vb=vb)\n",
    "#         approximation[k] = Q[k].evaluate(z, using=k, vb=vb)[1]\n",
    "        \n",
    "#     # Store approximations:\n",
    "#     result['KLD'] = KLD\n",
    "#     result['RMSE'] = RMSE\n",
    "#     result['approximation'] = approximation\n",
    "#     result['samples'] = Q['samples'].samples\n",
    "    \n",
    "#     with open(logfilename, 'a') as logfile:\n",
    "#         logfile.write(str((index, timeit.default_timer() - start_time))+'\\n')\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now lets's collate the metrics for the first 100 galaxies over a variable number of parameters, and look at the distribution of metric values.  We're using multiprocessing because the `for` loop is slow; the rate-limiting step is the optimization routine for finding quantiles of a GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_analysis(N):\n",
    "    \n",
    "#     all_results[str(N)] = []\n",
    "    \n",
    "#     pr = cProfile.Profile()\n",
    "#     pr.enable()\n",
    "    \n",
    "# # with qp.Ensemble\n",
    "#     n_gals_tot = len(pdfs)\n",
    "#     full_gal_range = range(n_gals_tot)\n",
    "#     subset = np.random.choice(full_gal_range, n_gals)\n",
    "#     pdfs_use = pdfs[subset]\n",
    "#     all_results[str(N)] = analyze(pdfs_use, nc_needed, z, N)\n",
    "\n",
    "# # # if multiprocessing:\n",
    "# #     logfilename = dataname + str(n_gals) + 'multi' + str(N)+'.txt'\n",
    "# #     def help_analyze(i):\n",
    "# #         return analyze_one(i, nc_needed, z, N, logfilename=logfilename)\n",
    "# #     pool = Pool(psutil.cpu_count() - 1)\n",
    "# #     results = pool.map(help_analyze, range(n_gals))\n",
    "# #     all_results[str(N)] = results\n",
    "# # # tl;dr Tmax=270s for N_floats=3, 100 galaxies, 3 processors\n",
    "    \n",
    "# # # if looping:\n",
    "# #     logfilename = dataname + str(n_gals) + 'loop' + str(N)+'.txt'\n",
    "# #     for i in range(100):\n",
    "# #         all_results[str(N)].append(analyze_one(i, 2, z, N, logfilename=logfilename))\n",
    "# #         if i%10 == 0: print('.', end='')\n",
    "# # # tl;dr Tmax=352s for N_floats=3, 100 galaxies\n",
    "    \n",
    "#     pr.disable()\n",
    "#     s = StringIO.StringIO()\n",
    "#     sortby = 'cumtime'\n",
    "#     ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "#     ps.print_stats()\n",
    "#     print(N, s.getvalue())\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%time\n",
    "\n",
    "# float_numbers = [3]#, 10, 30, 100]\n",
    "# n_float_numbers = len(float_numbers)\n",
    "\n",
    "# # gal_numbers = [100]#, 1000, 10000]\n",
    "# # n_gal_numbers = len(gal_numbers)\n",
    "\n",
    "# # total_results ={}\n",
    "# # for M in gal_numbers:\n",
    "# #     n_gals = M\n",
    "# n_gals = 100\n",
    "# all_results = {}\n",
    "# for N in float_numbers:\n",
    "#     start_time = timeit.default_timer()\n",
    "#     one_analysis(N)\n",
    "# #     total_results[str(n_gals)] = all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the previous step is quite slow (on the order of 5 minutes per test of different numbers of parameters for my laptop), this is a good point to save the results.  We can load them from the file later and not remake them if we only want to do the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_results.hkl', 'w') as result_file: \n",
    "#     hickle.dump(all_results, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_results.hkl', 'r') as result_file: \n",
    "#     all_results = hickle.load(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = total_results[str(gal_numbers[0])]\n",
    "\n",
    "# all_KLD, all_RMSE = [], []\n",
    "# for n in range(n_float_numbers):\n",
    "#     KLD, RMSE = {}, {}\n",
    "#     for approximation in all_results[str(float_numbers[n])][0]['KLD'].keys():\n",
    "#         x = np.array([])\n",
    "#         for k in range(len(all_results[str(float_numbers[n])])):\n",
    "#             x = np.append(x, all_results[str(float_numbers[n])][k]['KLD'][approximation])\n",
    "#         KLD[approximation] = x\n",
    "#         x = np.array([])\n",
    "#         for k in range(len(all_results[str(float_numbers[n])])):\n",
    "#             x = np.append(x, all_results[str(float_numbers[n])][k]['RMSE'][approximation])\n",
    "#         RMSE[approximation] = x\n",
    "#     all_KLD.append(KLD)\n",
    "#     all_RMSE.append(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot histograms of the metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = {'samples':'green', 'quantiles':'blue', 'histogram':'red'}\n",
    "# plt.figure(figsize=(12, 5 * n_float_numbers))\n",
    "\n",
    "# i=0\n",
    "# for n in range(n_float_numbers):\n",
    "#     i += 1\n",
    "#     # Lefthand panel: KLD\n",
    "#     plt.subplot(n_float_numbers, 2, i)\n",
    "#     plt.title('KLD for '+str(float_numbers[n])+' stored numbers')\n",
    "#     bins = np.linspace(0.0, 5., 25)\n",
    "#     for k in ['samples', 'quantiles', 'histogram']:\n",
    "#         plt.hist(all_KLD[n][k], bins, label=k, fc=colors[k], ec=colors[k], alpha=0.3, normed=True)\n",
    "#     #plt.semilogx()\n",
    "#     plt.xlabel('KL Divergence Metric', fontsize=16)\n",
    "#     plt.ylim(0., 5.0)\n",
    "#     plt.xlim(0., 5.0)\n",
    "#     plt.legend()\n",
    "    \n",
    "#     i += 1\n",
    "#     # Righthand panel: RMSE\n",
    "#     plt.subplot(n_float_numbers, 2, i)#+n_numbers)\n",
    "#     plt.title('RMSE for '+str(float_numbers[n])+' stored numbers')\n",
    "#     bins = np.linspace(0.0, 5., 25)\n",
    "#     for k in ['samples', 'quantiles', 'histogram']:\n",
    "#         plt.hist(all_RMSE[n][k], bins, label=k, fc=colors[k], ec=colors[k], alpha=0.3, normed=True)\n",
    "#     #plt.semilogx()\n",
    "#     plt.xlabel('RMS Error Metric', fontsize=16)\n",
    "#     plt.ylim(0., 5.0)\n",
    "#     plt.xlim(0., 5.0)\n",
    "#     plt.legend();\n",
    "    \n",
    "# plt.savefig('money.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the metrics don't agree, nor is the behavior consistent across different numbers of parameters.  However, as the number of parameters increases, the distribution of the metrics converge to lower numbers.\n",
    "\n",
    "KLD seems to flag more \"bad\" approximations than RMSE. How do we know where to set the threshold in each metric? \n",
    "\n",
    "We should think of the right way to get a summary statistic (first moment?) on the ensemble of KLD or RMSE values so we can make the plot of number of parameters vs. quality of approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the estimated $n(z)$. We'll do this with the GMM \"truth\", and then using each of our approximations. And we'll normalize the $n(z)$ to account for lost systems with bad approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5 * n_float_numbers))\n",
    "# all_n = []\n",
    "# all_x = []\n",
    "# all_y = []\n",
    "\n",
    "# for i in range(n_float_numbers):\n",
    "#     results = all_results[str(float_numbers[i])]\n",
    "#     n = {}\n",
    "\n",
    "#     # Pull out all truths and compute the average at each z:\n",
    "#     x = np.zeros([len(z), len(results)])\n",
    "#     y = {}\n",
    "#     for approx in ['samples', 'quantiles', 'histogram']:\n",
    "#         y[approx] = np.zeros([len(z), len(results)])\n",
    "#         for k in range(len(results)):\n",
    "#             y[approx][:,k] = results[k]['approximation'][approx] \n",
    "#     for k in range(len(results)):\n",
    "#         x[:,k] = results[k]['truth'] \n",
    "\n",
    "#     # Now do the averaging to make the estimators:\n",
    "#     n['truth'] = np.mean(x, axis=1)\n",
    "#     n['truth'] /= np.sum(n['truth']) * delta_z\n",
    "#     for approx in ['samples', 'quantiles', 'histogram']:\n",
    "#         n[approx] = np.mean(y[approx], axis=1)\n",
    "#         n[approx] /= np.sum(n[approx]) * delta_z\n",
    "        \n",
    "#     all_n.append(n)\n",
    "#     all_x.append(x)\n",
    "#     all_y.append(y)\n",
    "\n",
    "#     # Note: this uses the samples' KDE to make the approximation. We could (and \n",
    "#     # should!) also try simply concatenating the samples and histogramming them.\n",
    "    \n",
    "#     # Plot truth and all the approximations. \n",
    "#     # The NaNs in the histogram approximation make that unplottable for now.\n",
    "#     plt.subplot(n_float_numbers, 1, i+1)#+n_numbers)\n",
    "#     plt.title(r'$n(z)$ for '+str(float_numbers[i])+' numbers')\n",
    "#     plt.plot(z, n['truth'], color='black', lw=4, alpha=0.3, label='truth')\n",
    "#     for k in ['samples', 'quantiles', 'histogram']:\n",
    "#         plt.plot(z, n[k], label=k, color=colors[k])\n",
    "#     plt.xlabel('redshift z')\n",
    "#     plt.ylabel('n(z)')\n",
    "#     plt.legend();\n",
    "# plt.savefig('nz_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"samples\" approximation gives the best result for the $n(z)$ estimator even with a small number of samples.  However, once the number of parameters increases slightly, the \"quantiles\" approximation performs similarly.  It takes a large number of parameters before the \"histogram\" approximation approaches the other options. Let's use the `qp.PDF` object to compare them quantitatively (since $n(z)$ can be normalized to give the global $p(z)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_p = []\n",
    "\n",
    "# for i in range(n_float_numbers):\n",
    "#     n = all_n[i]\n",
    "#     p = {}\n",
    "#     for k in ['samples', 'quantiles', 'histogram']:\n",
    "#         p[k] = qp.PDF(gridded=(z,n[k]), vb=False)\n",
    "\n",
    "#     p['truth'] = qp.PDF(gridded=(z,n['truth']), vb=False)\n",
    "    \n",
    "#     all_p.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_KLD_nz, all_RMSE_nz = {}, {}\n",
    "# zlimits, dz = [z_low, z_high], 0.01\n",
    "# for k in ['samples', 'quantiles', 'histogram']:\n",
    "#     p = all_p[i]\n",
    "#     KLD_nz, RMSE_nz = [], []\n",
    "#     for i in range(n_float_numbers):\n",
    "#         KLD_nz.append(qp.calculate_kl_divergence(all_p[i]['truth'], all_p[i][k], limits=zlimits, dx=dz, vb=False))\n",
    "#         RMSE_nz.append(qp.calculate_rmse(all_p[i]['truth'], all_p[i][k], limits=zlimits, dx=dz, vb=False))\n",
    "    \n",
    "#     all_KLD_nz[k] = KLD_nz\n",
    "#     all_RMSE_nz[k] = RMSE_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 5))\n",
    "# both = [plt.subplot(1, 2, i+1) for i in range(2)]\n",
    "# KLD_plot = both[0]\n",
    "# RMSE_plot = both[1]\n",
    "# KLD_plot.set_title(r'KLD for $n(z)$')\n",
    "# RMSE_plot.set_title(r'RMSE for $n(z)$')\n",
    "# KLD_plot.set_xlabel('number of parameters')\n",
    "# RMSE_plot.set_xlabel('number of parameters')\n",
    "# KLD_plot.set_ylabel('KLD')\n",
    "# RMSE_plot.set_ylabel('RMSE')\n",
    "# # KLD_plot.semilogx()\n",
    "# # KLD_plot.semilogy()\n",
    "# # RMSE_plot.semilogx()\n",
    "# # RMSE_plot.semilogy()\n",
    "\n",
    "# for k in ['samples', 'quantiles', 'histogram']:\n",
    "#     KLD_plot.plot(float_numbers, all_KLD_nz[k], color=colors[k], label=k)\n",
    "#     RMSE_plot.plot(float_numbers, all_RMSE_nz[k], color=colors[k], label=k)\n",
    "\n",
    "# KLD_plot.semilogy()\n",
    "# KLD_plot.semilogx()\n",
    "# RMSE_plot.semilogy()\n",
    "# RMSE_plot.semilogx()\n",
    "# KLD_plot.legend()\n",
    "# RMSE_plot.legend()\n",
    "# plt.savefig('summary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('KLD metrics for n(z) estimator: ', all_KLD_nz)\n",
    "# print('RMSE metrics for n(z) estimator: ', all_RMSE_nz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This early indication suggests that quantiles perform best on the cleaner data set.\n",
    "\n",
    "A bigger test, using the full dataset, should allow this to be tested further: jack-knife error bars should also be calculable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no really, Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
